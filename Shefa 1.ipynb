{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5d7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "from read_write_qrcd import *\n",
    "from assets import *\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import sentencepiece\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from data_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f9e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 710 records from datasets/qrcd_v1.1_train.jsonl\n",
      "pq_id: 2:8-16_364\n",
      "pq_id: 2:174-176_364\n",
      "pq_id: 14:47-52_364\n",
      "pq_id: 17:12-17_364\n",
      "pq_id: 18:29-31_364\n",
      "pq_id: 27:89-93_364\n",
      "pq_id: 30:38-41_364\n",
      "pq_id: 39:38-41_364\n",
      "pq_id: 91:1-10_364\n",
      "pq_id: 2:8-16_378\n",
      "pq_id: 2:174-176_378\n",
      "pq_id: 2:277-281_378\n",
      "pq_id: 2:284-286_378\n",
      "pq_id: 5:48-50_378\n",
      "pq_id: 6:27-32_378\n",
      "pq_id: 6:100-105_378\n",
      "pq_id: 6:148-150_378\n",
      "pq_id: 14:47-52_378\n",
      "pq_id: 16:35-40_378\n",
      "pq_id: 17:12-17_378\n",
      "pq_id: 18:29-31_378\n",
      "pq_id: 27:89-93_378\n",
      "pq_id: 30:38-41_378\n",
      "pq_id: 35:15-18_378\n",
      "pq_id: 39:38-41_378\n",
      "pq_id: 41:45-48_378\n",
      "pq_id: 42:27-31_378\n",
      "pq_id: 45:12-15_378\n",
      "pq_id: 47:1-6_378\n",
      "pq_id: 67:1-5_378\n",
      "pq_id: 73:15-19_378\n",
      "pq_id: 74:32-48_378\n",
      "pq_id: 76:23-31_378\n",
      "pq_id: 78:38-40_378\n",
      "pq_id: 81:15-29_378\n",
      "pq_id: 91:1-10_378\n",
      "pq_id: 2:17-20_343\n",
      "pq_id: 10:3-6_343\n",
      "pq_id: 24:35-38_343\n",
      "pq_id: 2:25-25_143\n",
      "pq_id: 55:46-78_143\n",
      "pq_id: 56:11-26_143\n",
      "pq_id: 56:27-40_143\n",
      "pq_id: 2:26-27_231\n",
      "pq_id: 2:49-52_231\n",
      "pq_id: 2:53-57_231\n",
      "pq_id: 2:63-66_231\n",
      "pq_id: 2:67-73_231\n",
      "pq_id: 2:91-93_231\n",
      "pq_id: 2:172-173_231\n",
      "pq_id: 2:259-259_231\n",
      "pq_id: 2:260-260_231\n",
      "pq_id: 3:14-17_231\n",
      "pq_id: 4:153-161_231\n",
      "pq_id: 5:3-3_231\n",
      "pq_id: 5:4-5_231\n",
      "pq_id: 5:27-31_231\n",
      "pq_id: 5:59-60_231\n",
      "pq_id: 6:36-41_231\n",
      "pq_id: 6:143-144_231\n",
      "pq_id: 6:145-147_231\n",
      "pq_id: 7:73-79_231\n",
      "pq_id: 7:103-108_231\n",
      "pq_id: 7:130-136_231\n",
      "pq_id: 7:150-153_231\n",
      "pq_id: 7:159-162_231\n",
      "pq_id: 7:163-166_231\n",
      "pq_id: 7:175-178_231\n",
      "pq_id: 8:55-61_231\n",
      "pq_id: 11:61-68_231\n",
      "pq_id: 11:69-76_231\n",
      "pq_id: 12:11-15_231\n",
      "pq_id: 12:16-18_231\n",
      "pq_id: 12:36-42_231\n",
      "pq_id: 12:43-49_231\n",
      "pq_id: 12:63-66_231\n",
      "pq_id: 12:69-76_231\n",
      "pq_id: 16:1-9_231\n",
      "pq_id: 16:65-69_231\n",
      "pq_id: 16:77-79_231\n",
      "pq_id: 16:114-119_231\n",
      "pq_id: 17:59-60_231\n",
      "pq_id: 18:17-20_231\n",
      "pq_id: 18:21-22_231\n",
      "pq_id: 18:60-77_231\n",
      "pq_id: 20:17-24_231\n",
      "pq_id: 20:80-82_231\n",
      "pq_id: 21:78-82_231\n",
      "pq_id: 21:87-88_231\n",
      "pq_id: 22:30-37_231\n",
      "pq_id: 22:73-76_231\n",
      "pq_id: 24:41-45_231\n",
      "pq_id: 26:23-40_231\n",
      "pq_id: 26:141-159_231\n",
      "pq_id: 27:15-22_231\n",
      "pq_id: 27:20-28_231\n",
      "pq_id: 27:82-88_231\n",
      "pq_id: 29:41-43_231\n",
      "pq_id: 31:12-19_231\n",
      "pq_id: 34:10-13_231\n",
      "pq_id: 34:14-14_231\n",
      "pq_id: 37:139-148_231\n",
      "pq_id: 38:12-20_231\n",
      "pq_id: 51:24-37_231\n",
      "pq_id: 54:1-8_231\n",
      "pq_id: 54:23-32_231\n",
      "pq_id: 56:11-26_231\n",
      "pq_id: 59:6-7_231\n",
      "pq_id: 62:5-8_231\n",
      "pq_id: 67:16-23_231\n",
      "pq_id: 68:48-52_231\n",
      "pq_id: 74:49-56_231\n",
      "pq_id: 77:29-40_231\n",
      "pq_id: 88:17-26_231\n",
      "pq_id: 91:1-15_231\n",
      "pq_id: 101:1-11_231\n",
      "pq_id: 105:1-5_231\n",
      "pq_id: 2:30-33_164\n",
      "pq_id: 3:33-41_164\n",
      "pq_id: 7:142-143_164\n",
      "pq_id: 7:204-206_164\n",
      "pq_id: 10:7-10_164\n",
      "pq_id: 13:12-15_164\n",
      "pq_id: 17:40-44_164\n",
      "pq_id: 17:105-111_164\n",
      "pq_id: 21:16-20_164\n",
      "pq_id: 21:78-82_164\n",
      "pq_id: 24:35-38_164\n",
      "pq_id: 24:41-45_164\n",
      "pq_id: 32:15-17_164\n",
      "pq_id: 37:139-148_164\n",
      "pq_id: 37:158-169_164\n",
      "pq_id: 38:12-20_164\n",
      "pq_id: 39:71-75_164\n",
      "pq_id: 40:7-9_164\n",
      "pq_id: 41:37-39_164\n",
      "pq_id: 42:1-6_164\n",
      "pq_id: 57:1-6_164\n",
      "pq_id: 59:1-5_164\n",
      "pq_id: 59:21-24_164\n",
      "pq_id: 61:1-4_164\n",
      "pq_id: 62:1-4_164\n",
      "pq_id: 64:1-4_164\n",
      "pq_id: 2:49-52_413\n",
      "pq_id: 7:138-141_413\n",
      "pq_id: 14:5-8_413\n",
      "pq_id: 2:60-62_220\n",
      "pq_id: 10:87-89_220\n",
      "pq_id: 12:19-22_220\n",
      "pq_id: 12:97-101_220\n",
      "pq_id: 2:63-66_240\n",
      "pq_id: 7:163-166_240\n",
      "pq_id: 2:63-66_248\n",
      "pq_id: 4:153-161_248\n",
      "pq_id: 7:163-166_248\n",
      "pq_id: 2:74-75_407\n",
      "pq_id: 2:78-79_407\n",
      "pq_id: 3:78-80_407\n",
      "pq_id: 4:44-46_407\n",
      "pq_id: 5:12-13_407\n",
      "pq_id: 5:41-43_407\n",
      "pq_id: 6:91-92_407\n",
      "pq_id: 9:30-31_407\n",
      "pq_id: 2:102-103_232\n",
      "pq_id: 3:95-97_232\n",
      "pq_id: 5:20-26_232\n",
      "pq_id: 9:69-70_232\n",
      "pq_id: 9:101-106_232\n",
      "pq_id: 9:117-121_232\n",
      "pq_id: 11:84-94_232\n",
      "pq_id: 11:89-95_232\n",
      "pq_id: 20:38-48_232\n",
      "pq_id: 22:42-46_232\n",
      "pq_id: 23:17-22_232\n",
      "pq_id: 27:20-28_232\n",
      "pq_id: 28:22-28_232\n",
      "pq_id: 28:44-46_232\n",
      "pq_id: 29:36-40_232\n",
      "pq_id: 33:9-17_232\n",
      "pq_id: 33:60-62_232\n",
      "pq_id: 34:15-19_232\n",
      "pq_id: 48:24-26_232\n",
      "pq_id: 63:5-8_232\n",
      "pq_id: 89:1-14_232\n",
      "pq_id: 2:124-129_352\n",
      "pq_id: 2:130-134_352\n",
      "pq_id: 2:135-138_352\n",
      "pq_id: 3:62-68_352\n",
      "pq_id: 3:84-85_352\n",
      "pq_id: 10:71-73_352\n",
      "pq_id: 12:97-101_352\n",
      "pq_id: 27:38-44_352\n",
      "pq_id: 2:130-134_419\n",
      "pq_id: 2:135-138_419\n",
      "pq_id: 2:177-177_419\n",
      "pq_id: 2:213-214_419\n",
      "pq_id: 2:253-254_419\n",
      "pq_id: 2:284-286_419\n",
      "pq_id: 3:18-22_419\n",
      "pq_id: 3:33-41_419\n",
      "pq_id: 3:84-85_419\n",
      "pq_id: 4:66-70_419\n",
      "pq_id: 4:135-136_419\n",
      "pq_id: 4:150-152_419\n",
      "pq_id: 6:8-11_419\n",
      "pq_id: 6:84-90_419\n",
      "pq_id: 11:69-76_419\n",
      "pq_id: 13:30-34_419\n",
      "pq_id: 17:53-56_419\n",
      "pq_id: 19:41-51_419\n",
      "pq_id: 19:51-58_419\n",
      "pq_id: 21:36-41_419\n",
      "pq_id: 21:69-73_419\n",
      "pq_id: 33:7-9_419\n",
      "pq_id: 37:75-82_419\n",
      "pq_id: 38:30-40_419\n",
      "pq_id: 38:41-48_419\n",
      "pq_id: 40:47-52_419\n",
      "pq_id: 58:14-21_419\n",
      "pq_id: 2:151-153_414\n",
      "pq_id: 3:30-32_414\n",
      "pq_id: 3:130-136_414\n",
      "pq_id: 3:164-165_414\n",
      "pq_id: 4:12-14_414\n",
      "pq_id: 4:58-59_414\n",
      "pq_id: 4:64-65_414\n",
      "pq_id: 4:66-70_414\n",
      "pq_id: 4:80-84_414\n",
      "pq_id: 5:90-93_414\n",
      "pq_id: 5:103-105_414\n",
      "pq_id: 7:157-158_414\n",
      "pq_id: 8:20-26_414\n",
      "pq_id: 9:71-72_414\n",
      "pq_id: 24:46-54_414\n",
      "pq_id: 24:55-57_414\n",
      "pq_id: 24:62-64_414\n",
      "pq_id: 33:21-24_414\n",
      "pq_id: 33:36-40_414\n",
      "pq_id: 33:69-71_414\n",
      "pq_id: 47:33-38_414\n",
      "pq_id: 48:16-17_414\n",
      "pq_id: 58:12-13_414\n",
      "pq_id: 59:6-7_414\n",
      "pq_id: 64:11-13_414\n",
      "pq_id: 2:159-162_310\n",
      "pq_id: 3:86-91_310\n",
      "pq_id: 4:15-16_310\n",
      "pq_id: 4:17-18_310\n",
      "pq_id: 4:92-93_310\n",
      "pq_id: 4:144-147_310\n",
      "pq_id: 5:38-40_310\n",
      "pq_id: 6:53-58_310\n",
      "pq_id: 7:150-153_310\n",
      "pq_id: 9:1-6_310\n",
      "pq_id: 9:7-11_310\n",
      "pq_id: 11:1-5_310\n",
      "pq_id: 11:50-60_310\n",
      "pq_id: 16:114-119_310\n",
      "pq_id: 19:59-65_310\n",
      "pq_id: 20:80-82_310\n",
      "pq_id: 24:2-5_310\n",
      "pq_id: 25:63-77_310\n",
      "pq_id: 28:60-67_310\n",
      "pq_id: 58:12-13_310\n",
      "pq_id: 66:8-9_310\n",
      "pq_id: 2:177-177_325\n",
      "pq_id: 2:189-190_325\n",
      "pq_id: 3:92-94_325\n",
      "pq_id: 2:177-177_326\n",
      "pq_id: 2:189-190_326\n",
      "pq_id: 3:92-94_326\n",
      "pq_id: 19:12-15_326\n",
      "pq_id: 19:16-34_326\n",
      "pq_id: 58:7-10_326\n",
      "pq_id: 60:8-9_326\n",
      "pq_id: 76:5-11_326\n",
      "pq_id: 2:178-179_140\n",
      "pq_id: 4:92-93_140\n",
      "pq_id: 25:63-77_140\n",
      "pq_id: 2:190-194_396\n",
      "pq_id: 2:216-218_396\n",
      "pq_id: 2:196-203_311\n",
      "pq_id: 3:159-160_311\n",
      "pq_id: 14:24-27_311\n",
      "pq_id: 20:38-48_311\n",
      "pq_id: 22:23-24_311\n",
      "pq_id: 33:69-71_311\n",
      "pq_id: 35:9-11_311\n",
      "pq_id: 39:11-20_311\n",
      "pq_id: 2:216-218_303\n",
      "pq_id: 3:121-129_303\n",
      "pq_id: 3:166-168_303\n",
      "pq_id: 4:77-79_303\n",
      "pq_id: 5:35-37_303\n",
      "pq_id: 5:54-56_303\n",
      "pq_id: 8:62-66_303\n",
      "pq_id: 8:72-75_303\n",
      "pq_id: 9:19-22_303\n",
      "pq_id: 9:42-49_303\n",
      "pq_id: 9:73-74_303\n",
      "pq_id: 9:86-89_303\n",
      "pq_id: 49:14-18_303\n",
      "pq_id: 60:1-3_303\n",
      "pq_id: 61:10-14_303\n",
      "pq_id: 66:8-9_303\n",
      "pq_id: 2:216-218_408\n",
      "pq_id: 3:84-85_408\n",
      "pq_id: 3:86-91_408\n",
      "pq_id: 9:73-74_408\n",
      "pq_id: 47:25-29_408\n",
      "pq_id: 2:228-230_123\n",
      "pq_id: 33:49-52_123\n",
      "pq_id: 65:4-7_123\n",
      "pq_id: 2:228-230_424\n",
      "pq_id: 2:231-232_424\n",
      "pq_id: 2:234-237_424\n",
      "pq_id: 2:240-242_424\n",
      "pq_id: 3:190-195_424\n",
      "pq_id: 4:7-10_424\n",
      "pq_id: 4:19-21_424\n",
      "pq_id: 4:25-25_424\n",
      "pq_id: 4:32-33_424\n",
      "pq_id: 4:122-126_424\n",
      "pq_id: 4:127-127_424\n",
      "pq_id: 16:94-97_424\n",
      "pq_id: 33:35-35_424\n",
      "pq_id: 33:56-59_424\n",
      "pq_id: 40:38-46_424\n",
      "pq_id: 58:1-4_424\n",
      "pq_id: 60:12-13_424\n",
      "pq_id: 65:1-3_424\n",
      "pq_id: 65:4-7_424\n",
      "pq_id: 2:234-237_314\n",
      "pq_id: 3:133-136_314\n",
      "pq_id: 3:145-148_314\n",
      "pq_id: 3:169-175_314\n",
      "pq_id: 4:122-126_314\n",
      "pq_id: 4:128-130_314\n",
      "pq_id: 5:12-13_314\n",
      "pq_id: 5:82-86_314\n",
      "pq_id: 5:90-93_314\n",
      "pq_id: 7:54-56_314\n",
      "pq_id: 9:90-96_314\n",
      "pq_id: 9:117-121_314\n",
      "pq_id: 10:26-27_314\n",
      "pq_id: 11:112-115_314\n",
      "pq_id: 12:88-92_314\n",
      "pq_id: 16:30-32_314\n",
      "pq_id: 16:125-128_314\n",
      "pq_id: 18:29-31_314\n",
      "pq_id: 22:30-37_314\n",
      "pq_id: 28:76-81_314\n",
      "pq_id: 29:61-69_314\n",
      "pq_id: 33:28-31_314\n",
      "pq_id: 37:75-82_314\n",
      "pq_id: 37:99-113_314\n",
      "pq_id: 37:114-122_314\n",
      "pq_id: 37:123-132_314\n",
      "pq_id: 39:9-10_314\n",
      "pq_id: 39:32-37_314\n",
      "pq_id: 41:33-36_314\n",
      "pq_id: 53:31-41_314\n",
      "pq_id: 2:249-252_216\n",
      "pq_id: 4:163-166_216\n",
      "pq_id: 17:53-56_216\n",
      "pq_id: 21:78-82_216\n",
      "pq_id: 27:15-22_216\n",
      "pq_id: 34:10-13_216\n",
      "pq_id: 38:12-20_216\n",
      "pq_id: 38:21-26_216\n",
      "pq_id: 38:30-40_216\n",
      "pq_id: 2:249-252_258\n",
      "pq_id: 2:256-257_393\n",
      "pq_id: 18:29-31_393\n",
      "pq_id: 27:89-93_393\n",
      "pq_id: 39:38-41_393\n",
      "pq_id: 74:49-56_393\n",
      "pq_id: 76:23-31_393\n",
      "pq_id: 78:38-40_393\n",
      "pq_id: 109:1-6_393\n",
      "pq_id: 2:272-274_330\n",
      "pq_id: 2:277-281_330\n",
      "pq_id: 2:284-286_330\n",
      "pq_id: 3:176-180_330\n",
      "pq_id: 5:48-50_330\n",
      "pq_id: 5:54-56_330\n",
      "pq_id: 6:27-32_330\n",
      "pq_id: 6:100-105_330\n",
      "pq_id: 6:164-165_330\n",
      "pq_id: 11:6-7_330\n",
      "pq_id: 12:50-57_330\n",
      "pq_id: 14:47-52_330\n",
      "pq_id: 17:12-17_330\n",
      "pq_id: 18:29-31_330\n",
      "pq_id: 27:38-44_330\n",
      "pq_id: 27:89-93_330\n",
      "pq_id: 30:38-41_330\n",
      "pq_id: 31:12-19_330\n",
      "pq_id: 33:72-73_330\n",
      "pq_id: 35:15-18_330\n",
      "pq_id: 35:29-35_330\n",
      "pq_id: 39:38-41_330\n",
      "pq_id: 41:45-48_330\n",
      "pq_id: 42:27-31_330\n",
      "pq_id: 43:15-25_330\n",
      "pq_id: 45:12-15_330\n",
      "pq_id: 47:1-6_330\n",
      "pq_id: 67:1-5_330\n",
      "pq_id: 73:15-19_330\n",
      "pq_id: 74:32-48_330\n",
      "pq_id: 74:49-56_330\n",
      "pq_id: 76:23-31_330\n",
      "pq_id: 78:38-40_330\n",
      "pq_id: 79:34-41_330\n",
      "pq_id: 81:15-29_330\n",
      "pq_id: 87:6-19_330\n",
      "pq_id: 90:5-10_330\n",
      "pq_id: 91:1-10_330\n",
      "pq_id: 2:275-276_116\n",
      "pq_id: 2:277-281_116\n",
      "pq_id: 2:282-283_412\n",
      "pq_id: 3:33-41_106\n",
      "pq_id: 3:33-41_137\n",
      "pq_id: 19:1-11_137\n",
      "pq_id: 21:89-90_137\n",
      "pq_id: 3:33-41_122\n",
      "pq_id: 19:1-11_122\n",
      "pq_id: 3:45-51_155\n",
      "pq_id: 5:44-47_155\n",
      "pq_id: 5:110-113_155\n",
      "pq_id: 57:25-27_155\n",
      "pq_id: 3:45-51_160\n",
      "pq_id: 4:153-161_160\n",
      "pq_id: 4:171-172_160\n",
      "pq_id: 5:15-17_160\n",
      "pq_id: 5:74-77_160\n",
      "pq_id: 3:45-51_219\n",
      "pq_id: 3:49-53_219\n",
      "pq_id: 5:72-73_219\n",
      "pq_id: 5:78-81_219\n",
      "pq_id: 5:112-115_219\n",
      "pq_id: 61:6-9_219\n",
      "pq_id: 61:10-14_219\n",
      "pq_id: 3:49-53_212\n",
      "pq_id: 5:110-113_212\n",
      "pq_id: 61:10-14_212\n",
      "pq_id: 3:49-53_266\n",
      "pq_id: 9:100-102_266\n",
      "pq_id: 9:117-121_266\n",
      "pq_id: 61:10-14_266\n",
      "pq_id: 3:54-58_404\n",
      "pq_id: 4:153-161_404\n",
      "pq_id: 5:116-120_404\n",
      "pq_id: 3:95-97_422\n",
      "pq_id: 5:20-26_422\n",
      "pq_id: 17:1-1_422\n",
      "pq_id: 20:9-16_422\n",
      "pq_id: 79:15-26_422\n",
      "pq_id: 3:121-129_265\n",
      "pq_id: 3:137-144_357\n",
      "pq_id: 4:95-96_357\n",
      "pq_id: 5:35-37_357\n",
      "pq_id: 5:54-56_357\n",
      "pq_id: 8:72-75_357\n",
      "pq_id: 9:19-22_357\n",
      "pq_id: 9:38-41_357\n",
      "pq_id: 9:86-89_357\n",
      "pq_id: 29:61-69_357\n",
      "pq_id: 3:187-189_402\n",
      "pq_id: 9:30-31_402\n",
      "pq_id: 4:2-6_136\n",
      "pq_id: 4:36-40_401\n",
      "pq_id: 10:61-61_401\n",
      "pq_id: 34:3-5_401\n",
      "pq_id: 34:20-23_401\n",
      "pq_id: 99:1-8_401\n",
      "pq_id: 4:80-84_421\n",
      "pq_id: 10:37-44_421\n",
      "pq_id: 15:1-9_421\n",
      "pq_id: 17:105-111_421\n",
      "pq_id: 39:27-31_421\n",
      "pq_id: 41:40-44_421\n",
      "pq_id: 43:1-8_421\n",
      "pq_id: 75:16-25_421\n",
      "pq_id: 85:12-22_421\n",
      "pq_id: 4:95-96_356\n",
      "pq_id: 8:72-75_356\n",
      "pq_id: 9:19-22_356\n",
      "pq_id: 9:38-41_356\n",
      "pq_id: 9:42-49_356\n",
      "pq_id: 9:73-74_356\n",
      "pq_id: 9:81-85_356\n",
      "pq_id: 9:86-89_356\n",
      "pq_id: 47:1-6_356\n",
      "pq_id: 49:14-18_356\n",
      "pq_id: 61:10-14_356\n",
      "pq_id: 66:8-9_356\n",
      "pq_id: 4:101-103_362\n",
      "pq_id: 7:80-84_362\n",
      "pq_id: 8:30-37_362\n",
      "pq_id: 11:77-83_362\n",
      "pq_id: 15:61-77_362\n",
      "pq_id: 25:35-40_362\n",
      "pq_id: 26:160-175_362\n",
      "pq_id: 27:54-58_362\n",
      "pq_id: 46:21-25_362\n",
      "pq_id: 4:114-115_342\n",
      "pq_id: 7:204-206_342\n",
      "pq_id: 22:25-29_342\n",
      "pq_id: 31:12-19_342\n",
      "pq_id: 43:74-80_342\n",
      "pq_id: 49:1-5_342\n",
      "pq_id: 58:7-10_342\n",
      "pq_id: 68:48-52_342\n",
      "pq_id: 5:4-5_410\n",
      "pq_id: 5:82-86_410\n",
      "pq_id: 60:8-9_410\n",
      "pq_id: 5:6-6_380\n",
      "pq_id: 5:6-6_379\n",
      "pq_id: 5:27-31_205\n",
      "pq_id: 5:38-40_118\n",
      "pq_id: 5:51-53_426\n",
      "pq_id: 5:57-58_426\n",
      "pq_id: 5:82-86_341\n",
      "pq_id: 7:204-206_341\n",
      "pq_id: 39:11-20_341\n",
      "pq_id: 50:36-45_341\n",
      "pq_id: 62:9-11_341\n",
      "pq_id: 5:89-89_147\n",
      "pq_id: 5:101-102_385\n",
      "pq_id: 5:106-108_202\n",
      "pq_id: 6:80-86_204\n",
      "pq_id: 11:69-76_204\n",
      "pq_id: 14:35-41_204\n",
      "pq_id: 19:41-51_204\n",
      "pq_id: 37:99-113_204\n",
      "pq_id: 6:84-90_371\n",
      "pq_id: 40:34-35_371\n",
      "pq_id: 6:125-127_405\n",
      "pq_id: 7:54-56_152\n",
      "pq_id: 10:3-6_152\n",
      "pq_id: 11:6-7_152\n",
      "pq_id: 25:56-62_152\n",
      "pq_id: 32:4-9_152\n",
      "pq_id: 41:9-12_152\n",
      "pq_id: 50:36-45_152\n",
      "pq_id: 57:1-6_152\n",
      "pq_id: 7:73-79_115\n",
      "pq_id: 11:61-68_115\n",
      "pq_id: 17:59-60_115\n",
      "pq_id: 54:23-32_115\n",
      "pq_id: 91:1-15_115\n",
      "pq_id: 7:73-79_225\n",
      "pq_id: 11:61-68_225\n",
      "pq_id: 17:59-60_225\n",
      "pq_id: 54:23-32_225\n",
      "pq_id: 7:80-84_397\n",
      "pq_id: 26:160-175_397\n",
      "pq_id: 27:54-58_397\n",
      "pq_id: 29:28-35_397\n",
      "pq_id: 54:33-42_397\n",
      "pq_id: 7:85-93_101\n",
      "pq_id: 11:84-94_101\n",
      "pq_id: 26:176-191_101\n",
      "pq_id: 29:36-40_101\n",
      "pq_id: 7:85-93_245\n",
      "pq_id: 11:84-94_245\n",
      "pq_id: 26:176-191_245\n",
      "pq_id: 29:36-40_245\n",
      "pq_id: 7:103-108_102\n",
      "pq_id: 10:74-78_102\n",
      "pq_id: 11:96-99_102\n",
      "pq_id: 17:2-8_102\n",
      "pq_id: 17:101-104_102\n",
      "pq_id: 20:17-24_102\n",
      "pq_id: 20:38-48_102\n",
      "pq_id: 23:45-50_102\n",
      "pq_id: 26:10-22_102\n",
      "pq_id: 28:76-81_102\n",
      "pq_id: 29:38-40_102\n",
      "pq_id: 32:23-27_102\n",
      "pq_id: 40:23-27_102\n",
      "pq_id: 40:53-58_102\n",
      "pq_id: 43:46-56_102\n",
      "pq_id: 51:38-46_102\n",
      "pq_id: 79:15-26_102\n",
      "pq_id: 7:142-143_108\n",
      "pq_id: 19:51-58_108\n",
      "pq_id: 20:25-37_108\n",
      "pq_id: 20:86-94_108\n",
      "pq_id: 23:45-50_108\n",
      "pq_id: 25:35-40_108\n",
      "pq_id: 28:33-35_108\n",
      "pq_id: 9:12-16_350\n",
      "pq_id: 10:57-60_350\n",
      "pq_id: 16:65-69_350\n",
      "pq_id: 17:78-82_350\n",
      "pq_id: 26:69-89_350\n",
      "pq_id: 41:40-44_350\n",
      "pq_id: 9:36-37_125\n",
      "pq_id: 10:15-17_411\n",
      "pq_id: 15:1-9_411\n",
      "pq_id: 41:40-44_411\n",
      "pq_id: 75:16-25_411\n",
      "pq_id: 10:24-25_308\n",
      "pq_id: 18:45-46_308\n",
      "pq_id: 57:18-21_308\n",
      "pq_id: 10:87-89_227\n",
      "pq_id: 43:46-56_227\n",
      "pq_id: 11:41-44_129\n",
      "pq_id: 11:41-44_207\n",
      "pq_id: 11:45-48_207\n",
      "pq_id: 11:50-60_337\n",
      "pq_id: 55:37-45_337\n",
      "pq_id: 96:9-19_337\n",
      "pq_id: 12:1-3_159\n",
      "pq_id: 20:102-113_159\n",
      "pq_id: 39:27-31_159\n",
      "pq_id: 41:1-8_159\n",
      "pq_id: 41:40-44_159\n",
      "pq_id: 42:7-12_159\n",
      "pq_id: 43:1-8_159\n",
      "pq_id: 12:4-6_144\n",
      "pq_id: 12:67-69_144\n",
      "pq_id: 12:7-10_224\n",
      "pq_id: 12:23-33_128\n",
      "pq_id: 12:97-101_128\n",
      "pq_id: 12:39-54_237\n",
      "pq_id: 12:43-49_354\n",
      "pq_id: 29:14-18_354\n",
      "pq_id: 13:18-24_360\n",
      "pq_id: 40:7-9_360\n",
      "pq_id: 52:17-28_360\n",
      "pq_id: 16:84-89_389\n",
      "pq_id: 17:88-89_389\n",
      "pq_id: 18:54-56_389\n",
      "pq_id: 30:55-60_389\n",
      "pq_id: 39:27-31_389\n",
      "pq_id: 17:2-8_365\n",
      "pq_id: 17:101-104_365\n",
      "pq_id: 18:60-82_253\n",
      "pq_id: 18:83-98_209\n",
      "pq_id: 18:83-98_328\n",
      "pq_id: 21:96-100_328\n",
      "pq_id: 22:1-2_328\n",
      "pq_id: 54:1-8_328\n",
      "pq_id: 75:1-15_328\n",
      "pq_id: 19:12-15_327\n",
      "pq_id: 19:16-34_327\n",
      "pq_id: 76:5-11_327\n",
      "pq_id: 82:13-19_327\n",
      "pq_id: 83:18-28_327\n",
      "pq_id: 19:16-34_363\n",
      "pq_id: 19:51-58_361\n",
      "pq_id: 22:52-57_361\n",
      "pq_id: 33:36-40_361\n",
      "pq_id: 19:77-87_367\n",
      "pq_id: 20:102-113_367\n",
      "pq_id: 21:25-29_367\n",
      "pq_id: 40:7-9_367\n",
      "pq_id: 43:81-89_367\n",
      "pq_id: 53:19-30_367\n",
      "pq_id: 21:30-35_347\n",
      "pq_id: 22:58-66_347\n",
      "pq_id: 31:29-32_347\n",
      "pq_id: 35:12-14_347\n",
      "pq_id: 36:33-47_347\n",
      "pq_id: 39:1-5_347\n",
      "pq_id: 57:1-6_347\n",
      "pq_id: 21:78-82_120\n",
      "pq_id: 27:15-22_120\n",
      "pq_id: 21:83-86_105\n",
      "pq_id: 38:41-44_105\n",
      "pq_id: 21:87-88_249\n",
      "pq_id: 37:139-148_249\n",
      "pq_id: 68:48-52_249\n",
      "pq_id: 24:2-5_368\n",
      "pq_id: 24:6-10_368\n",
      "pq_id: 24:23-26_368\n",
      "pq_id: 24:30-31_301\n",
      "pq_id: 33:53-55_301\n",
      "pq_id: 33:56-59_301\n",
      "pq_id: 24:41-45_409\n",
      "pq_id: 27:15-22_409\n",
      "pq_id: 27:20-28_409\n",
      "pq_id: 27:82-88_409\n",
      "pq_id: 27:15-22_243\n",
      "pq_id: 28:76-81_114\n",
      "pq_id: 28:76-81_145\n",
      "pq_id: 29:38-40_145\n",
      "pq_id: 40:23-27_145\n",
      "pq_id: 29:46-47_427\n",
      "pq_id: 33:4-6_395\n",
      "pq_id: 37:62-74_119\n",
      "pq_id: 44:40-50_119\n",
      "pq_id: 56:41-56_119\n",
      "pq_id: 37:62-74_127\n",
      "pq_id: 44:40-50_127\n",
      "pq_id: 56:41-56_127\n",
      "pq_id: 37:139-148_138\n",
      "pq_id: 46:21-25_235\n",
      "pq_id: 89:1-14_235\n",
      "pq_id: 46:29-32_398\n",
      "pq_id: 72:1-7_398\n",
      "pq_id: 46:29-32_399\n",
      "pq_id: 72:1-7_399\n",
      "pq_id: 58:1-4_146\n",
      "pq_id: 83:1-17_141\n",
      "pq_id: 97:1-5_153\n",
      "pq_id: 101:1-11_109\n",
      "Collected 710 Object from datasets/qrcd_v1.1_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "passage_question_objects = read_JSONL_file('datasets/qrcd_v1.1_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dea468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "df_file = json_to_dataframe(passage_question_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a65f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2:8-16_364</td>\n",
       "      <td>ومن الناس من يقول امنا بالله وباليوم الاخر وما...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['اولئك الذين اشتروا الضلاله بالهدي']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2:174-176_364</td>\n",
       "      <td>ان الذين يكتمون ما انزل الله من الكتاب ويشترون...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['اولئك الذين اشتروا الضلاله بالهدي و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14:47-52_364</td>\n",
       "      <td>فلا تحسبن الله مخلف وعده رسله ان الله عزيز ذو ...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['ليجزي الله كل نفس ما كسبت'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17:12-17_364</td>\n",
       "      <td>وجعلنا الليل والنهار ايتين فمحونا ايه الليل وج...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['كل انسان الزمناه طائره في عنقه', 'م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18:29-31_364</td>\n",
       "      <td>وقل الحق من ربكم فمن شاء فليؤمن ومن شاء فليكفر...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['من شاء فليؤمن ومن شاء فليكفر'], 'an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            context  \\\n",
       "0     2:8-16_364  ومن الناس من يقول امنا بالله وباليوم الاخر وما...   \n",
       "1  2:174-176_364  ان الذين يكتمون ما انزل الله من الكتاب ويشترون...   \n",
       "2   14:47-52_364  فلا تحسبن الله مخلف وعده رسله ان الله عزيز ذو ...   \n",
       "3   17:12-17_364  وجعلنا الليل والنهار ايتين فمحونا ايه الليل وج...   \n",
       "4   18:29-31_364  وقل الحق من ربكم فمن شاء فليؤمن ومن شاء فليكفر...   \n",
       "\n",
       "                                            question  \\\n",
       "0  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "1  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "2  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "3  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "4  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['اولئك الذين اشتروا الضلاله بالهدي']...  \n",
       "1  {'text': ['اولئك الذين اشتروا الضلاله بالهدي و...  \n",
       "2  {'text': ['ليجزي الله كل نفس ما كسبت'], 'answe...  \n",
       "3  {'text': ['كل انسان الزمناه طائره في عنقه', 'م...  \n",
       "4  {'text': ['من شاء فليؤمن ومن شاء فليكفر'], 'an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77b90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbc7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a55dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2:8-16_364',\n",
       " 'context': 'ومن الناس من يقول امنا بالله وباليوم الاخر وما هم بمؤمنين . يخادعون الله والذين امنوا وما يخدعون الا انفسهم وما يشعرون . في قلوبهم مرض فزادهم الله مرضا ولهم عذاب اليم بما كانوا يكذبون . واذا قيل لهم لا تفسدوا في الارض قالوا انما نحن مصلحون . الا انهم هم المفسدون ولكن لا يشعرون . واذا قيل لهم امنوا كما امن الناس قالوا انؤمن كما امن السفهاء الا انهم هم السفهاء ولكن لا يعلمون . واذا لقوا الذين امنوا قالوا امنا واذا خلوا الي شياطينهم قالوا انا معكم انما نحن مستهزئون . الله يستهزئ بهم ويمدهم في طغيانهم يعمهون . اولئك الذين اشتروا الضلاله بالهدي فما ربحت تجارتهم وما كانوا مهتدين . ',\n",
       " 'question': 'لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من يضلل الله فما له من هاد كما ورد من قوله تعالي في ايه 23 و ايه 36 من سوره الزمر ؟ ',\n",
       " 'answers': {'answer_start': [504],\n",
       "  'text': ['اولئك الذين اشتروا الضلاله بالهدي']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47b1d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4b36e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:كتب عليكم القتال وهو كره لكم وعسي ان تكرهوا شيئا وهو خير لكم وعسي ان تحبوا شيئا وهو شر لكم والله يعلم وانتم لا تعلمون . يسالونك عن الشهر الحرام قتال فيه قل قتال فيه كبير وصد عن سبيل الله وكفر به والمسجد الحرام واخراج اهله منه اكبر عند الله والفتنه اكبر من القتل ولا يزالون يقاتلونكم حتي يردوكم عن دينكم ان استطاعوا ومن يرتدد منكم عن دينه فيمت وهو كافر فاولئك حبطت اعمالهم في الدنيا والاخره واولئك اصحاب النار هم فيها خالدون . ان الذين امنوا والذين هاجروا وجاهدوا في سبيل الله اولئك يرجون رحمت الله والله غفور رحيم . \n",
      "Question:ما حكم من يرتد عن دين الاسلام ؟ \n",
      "Answer:['من يرتدد منكم عن دينه فيمت وهو كافر فاولئك حبطت اعمالهم في الدنيا والاخره واولئك اصحاب النار هم فيها خالدون']\n",
      "Answer Start in Text:[315]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:واذ نجيناكم من ال فرعون يسومونكم سوء العذاب يذبحون ابناءكم ويستحيون نساءكم وفي ذلكم بلاء من ربكم عظيم . واذ فرقنا بكم البحر فانجيناكم واغرقنا ال فرعون وانتم تنظرون . واذ واعدنا موسي اربعين ليله ثم اتخذتم العجل من بعده وانتم ظالمون . ثم عفونا عنكم من بعد ذلك لعلكم تشكرون . \n",
      "Question:ما هي انواع الحيوانات التي ذكرت في القران ؟ \n",
      "Answer:['العجل']\n",
      "Answer Start in Text:[202]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ولئن سالتهم من خلق السماوات والارض ليقولن الله قل افرايتم ما تدعون من دون الله ان ارادني الله بضر هل هن كاشفات ضره او ارادني برحمه هل هن ممسكات رحمته قل حسبي الله عليه يتوكل المتوكلون . قل يا قوم اعملوا علي مكانتكم اني عامل فسوف تعلمون . من ياتيه عذاب يخزيه ويحل عليه عذاب مقيم . انا انزلنا عليك الكتاب للناس بالحق فمن اهتدي فلنفسه ومن ضل فانما يضل عليها وما انت عليهم بوكيل . \n",
      "Question:ما هي الدلائل التي تشير بان الانسان مخير ؟ \n",
      "Answer:['من اهتدي فلنفسه ومن ضل فانما يضل عليها']\n",
      "Answer Start in Text:[313]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:قل يا اهل الكتاب هل تنقمون منا الا ان امنا بالله وما انزل الينا وما انزل من قبل وان اكثركم فاسقون . قل هل انبئكم بشر من ذلك مثوبه عند الله من لعنه الله وغضب عليه وجعل منهم القرده والخنازير وعبد الطاغوت اولئك شر مكانا واضل عن سواء السبيل . \n",
      "Question:ما هي انواع الحيوانات التي ذكرت في القران ؟ \n",
      "Answer:['القرده', 'الخنازير']\n",
      "Answer Start in Text:[171, 179]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:يا ايها الذين امنوا كونوا قوامين بالقسط شهداء لله ولو علي انفسكم او الوالدين والاقربين ان يكن غنيا او فقيرا فالله اولي بهما فلا تتبعوا الهوي ان تعدلوا وان تلوا او تعرضوا فان الله كان بما تعملون خبيرا . يا ايها الذين امنوا امنوا بالله ورسوله والكتاب الذي نزل علي رسوله والكتاب الذي انزل من قبل ومن يكفر بالله وملائكته وكتبه ورسله واليوم الاخر فقد ضل ضلالا بعيدا . \n",
      "Question:هل احترم الاسلام الانبياء ؟ \n",
      "Answer:['من يكفر بالله وملائكته وكتبه ورسله واليوم الاخر فقد ضل ضلالا بعيدا']\n",
      "Answer Start in Text:[294]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:فرجع موسي الي قومه غضبان اسفا قال يا قوم الم يعدكم ربكم وعدا حسنا افطال عليكم العهد ام اردتم ان يحل عليكم غضب من ربكم فاخلفتم موعدي . قالوا ما اخلفنا موعدك بملكنا ولكنا حملنا اوزارا من زينه القوم فقذفناها فكذلك القي السامري . فاخرج لهم عجلا جسدا له خوار فقالوا هذا الهكم واله موسي فنسي . افلا يرون الا يرجع اليهم قولا ولا يملك لهم ضرا ولا نفعا . ولقد قال لهم هارون من قبل يا قوم انما فتنتم به وان ربكم الرحمن فاتبعوني واطيعوا امري . قالوا لن نبرح عليه عاكفين حتي يرجع الينا موسي . قال يا هارون ما منعك اذ رايتهم ضلوا . الا تتبعن افعصيت امري . قال يا ابن ام لا تاخذ بلحيتي ولا براسي اني خشيت ان تقول فرقت بين بني اسرائيل ولم ترقب قولي . \n",
      "Question:من هو اخو سيدنا موسي ؟ \n",
      "Answer:['هارون']\n",
      "Answer Start in Text:[482]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ولله غيب السماوات والارض وما امر الساعه الا كلمح البصر او هو اقرب ان الله علي كل شيء قدير . والله اخرجكم من بطون امهاتكم لا تعلمون شيئا وجعل لكم السمع والابصار والافئده لعلكم تشكرون . الم يروا الي الطير مسخرات في جو السماء ما يمسكهن الا الله ان في ذلك لايات لقوم يؤمنون . \n",
      "Question:ما هي انواع الحيوانات التي ذكرت في القران ؟ \n",
      "Answer:['الطير']\n",
      "Answer Start in Text:[195]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ولما جاءت رسلنا لوطا سيء بهم وضاق بهم ذرعا وقال هذا يوم عصيب . وجاءه قومه يهرعون اليه ومن قبل كانوا يعملون السيئات قال يا قوم هؤلاء بناتي هن اطهر لكم فاتقوا الله ولا تخزون في ضيفي اليس منكم رجل رشيد . قالوا لقد علمت ما لنا في بناتك من حق وانك لتعلم ما نريد . قال لو ان لي بكم قوه او اوي الي ركن شديد . قالوا يا لوط انا رسل ربك لن يصلوا اليك فاسر باهلك بقطع من الليل ولا يلتفت منكم احد الا امراتك انه مصيبها ما اصابهم ان موعدهم الصبح اليس الصبح بقريب . فلما جاء امرنا جعلنا عاليها سافلها وامطرنا عليها حجاره من سجيل منضود . مسومه عند ربك وما هي من الظالمين ببعيد . \n",
      "Question:هل استخدم لفظ ( المطر ) في القران للعذاب فقط ؟ \n",
      "Answer:['امطرنا عليها حجاره من سجيل منضود']\n",
      "Answer Start in Text:[483]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ثم ارسلنا موسي واخاه هارون باياتنا وسلطان مبين . الي فرعون وملئه فاستكبروا وكانوا قوما عالين . فقالوا انؤمن لبشرين مثلنا وقومهما لنا عابدون . فكذبوهما فكانوا من المهلكين . ولقد اتينا موسي الكتاب لعلهم يهتدون . وجعلنا ابن مريم وامه ايه واويناهما الي ربوه ذات قرار ومعين . \n",
      "Question:من هو اخو سيدنا موسي ؟ \n",
      "Answer:['هارون']\n",
      "Answer Start in Text:[21]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:وما ارسلنا من رسول الا ليطاع باذن الله ولو انهم اذ ظلموا انفسهم جاءوك فاستغفروا الله واستغفر لهم الرسول لوجدوا الله توابا رحيما . فلا وربك لا يؤمنون حتي يحكموك فيما شجر بينهم ثم لا يجدوا في انفسهم حرجا مما قضيت ويسلموا تسليما . \n",
      "Question:لماذا لا يكتفي المسلمون بالقران الكريم ويلجاون للسنه ايضا ؟ \n",
      "Answer:['فلا وربك لا يؤمنون حتي يحكموك فيما شجر بينهم ثم لا يجدوا في انفسهم حرجا مما قضيت ويسلموا تسليما']\n",
      "Answer Start in Text:[129]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# datasets = load_dataset(\"squad\")\n",
    "def visualize(datasets, n_questions=10):\n",
    "    n = len(datasets)\n",
    "    random_questions=random.choices(list(range(n)),k=n_questions)\n",
    "    \n",
    "    for i in random_questions:\n",
    "        print(f\"Context:{datasets[i]['context']}\")\n",
    "        print(f\"Question:{datasets[i]['question']}\")\n",
    "        print(f\"Answer:{datasets[i]['answers']['text']}\")\n",
    "        print(f\"Answer Start in Text:{datasets[i]['answers']['answer_start']}\")\n",
    "        print(\"-\"*100)\n",
    "visualize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51ec8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with long docs:\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" ],\n",
    "        examples[\"context\" ],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    # Looks like [0,1,2,2,2,3,4,5,5...] - Here 2nd input pair has been split in 3 parts\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    # Looks like [[(0,0),(0,3),(3,4)...] ] - Contains the actual start indices and end indices for each word in the input.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    print(\"First===================\")\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc4c89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'context', 'question', 'answers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1f075ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/abdelrahman/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/abdelrahman/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/abdelrahman/.cache/huggingface/transformers/tmp03u5ocrp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1ba053959a40829df339bf26aab1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /home/abdelrahman/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "creating metadata file for /home/abdelrahman/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/abdelrahman/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/abdelrahman/.cache/huggingface/transformers/tmpn0vldck0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ba1c98ee0f421cae5edb72820d5a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /home/abdelrahman/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "creating metadata file for /home/abdelrahman/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/abdelrahman/.cache/huggingface/transformers/tmpsrg_dokf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ff08b86eab4f33acd3fdb4b15a20cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /home/abdelrahman/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "creating metadata file for /home/abdelrahman/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/abdelrahman/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/abdelrahman/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/abdelrahman/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/abdelrahman/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"salti/AraElectra-base-finetuned-ARCD\"\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e8b2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9a8378db7f48689e7a00a585d915ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First===================\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66cb99fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3346,\n",
       " 20120,\n",
       " 1006,\n",
       " 225,\n",
       " 1845,\n",
       " 2552,\n",
       " 41902,\n",
       " 759,\n",
       " 4019,\n",
       " 195,\n",
       " 338,\n",
       " 418,\n",
       " 306,\n",
       " 7641,\n",
       " 3986,\n",
       " 647,\n",
       " 3671,\n",
       " 595,\n",
       " 306,\n",
       " 9237,\n",
       " 547,\n",
       " 3663,\n",
       " 306,\n",
       " 4151,\n",
       " 55497,\n",
       " 305,\n",
       " 14071,\n",
       " 2474,\n",
       " 139,\n",
       " 14071,\n",
       " 4337,\n",
       " 306,\n",
       " 16398,\n",
       " 195,\n",
       " 33992,\n",
       " 105,\n",
       " 3,\n",
       " 623,\n",
       " 1571,\n",
       " 306,\n",
       " 1721,\n",
       " 5330,\n",
       " 181,\n",
       " 10515,\n",
       " 2609,\n",
       " 883,\n",
       " 201,\n",
       " 11356,\n",
       " 1177,\n",
       " 1891,\n",
       " 28076,\n",
       " 9133,\n",
       " 20,\n",
       " 2186,\n",
       " 47787,\n",
       " 319,\n",
       " 647,\n",
       " 6767,\n",
       " 5330,\n",
       " 330,\n",
       " 1177,\n",
       " 2186,\n",
       " 36288,\n",
       " 319,\n",
       " 335,\n",
       " 15265,\n",
       " 1177,\n",
       " 16391,\n",
       " 20,\n",
       " 305,\n",
       " 25041,\n",
       " 3524,\n",
       " 34968,\n",
       " 22890,\n",
       " 647,\n",
       " 41924,\n",
       " 28825,\n",
       " 34064,\n",
       " 11763,\n",
       " 1199,\n",
       " 2476,\n",
       " 35029,\n",
       " 319,\n",
       " 20,\n",
       " 6762,\n",
       " 10007,\n",
       " 1493,\n",
       " 391,\n",
       " 52708,\n",
       " 330,\n",
       " 305,\n",
       " 6327,\n",
       " 9466,\n",
       " 9448,\n",
       " 2006,\n",
       " 28231,\n",
       " 319,\n",
       " 20,\n",
       " 335,\n",
       " 5014,\n",
       " 1891,\n",
       " 32758,\n",
       " 5326,\n",
       " 847,\n",
       " 391,\n",
       " 16391,\n",
       " 20,\n",
       " 6762,\n",
       " 10007,\n",
       " 1493,\n",
       " 5330,\n",
       " 330,\n",
       " 547,\n",
       " 5330,\n",
       " 1571,\n",
       " 9466,\n",
       " 338,\n",
       " 11148,\n",
       " 547,\n",
       " 5330,\n",
       " 1512,\n",
       " 2180,\n",
       " 335,\n",
       " 5014,\n",
       " 1891,\n",
       " 1512,\n",
       " 2180,\n",
       " 847,\n",
       " 391,\n",
       " 20132,\n",
       " 20,\n",
       " 6762,\n",
       " 25642,\n",
       " 860,\n",
       " 5330,\n",
       " 330,\n",
       " 9466,\n",
       " 5330,\n",
       " 181,\n",
       " 6762,\n",
       " 28615,\n",
       " 181,\n",
       " 487,\n",
       " 45772,\n",
       " 6571,\n",
       " 336,\n",
       " 9466,\n",
       " 6728,\n",
       " 23288,\n",
       " 9448,\n",
       " 2006,\n",
       " 587,\n",
       " 18239,\n",
       " 4399,\n",
       " 20,\n",
       " 647,\n",
       " 885,\n",
       " 18239,\n",
       " 233,\n",
       " 3905,\n",
       " 2825,\n",
       " 5695,\n",
       " 201,\n",
       " 305,\n",
       " 44009,\n",
       " 336,\n",
       " 29283,\n",
       " 15498,\n",
       " 20,\n",
       " 23566,\n",
       " 860,\n",
       " 2537,\n",
       " 10119,\n",
       " 792,\n",
       " 25596,\n",
       " 6334,\n",
       " 458,\n",
       " 3671,\n",
       " 15762,\n",
       " 228,\n",
       " 9914,\n",
       " 1446,\n",
       " 1177,\n",
       " 2476,\n",
       " 1746,\n",
       " 30861,\n",
       " 307,\n",
       " 20,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7afd21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 712\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 135\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-91f4270433c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test-squad-trained\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         )\n\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m         )\n\u001b[1;32m    996\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = default_data_collator\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(trainer.save_model(\"test-squad-trained\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5436b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/abdelrahman/.local/lib/python3.6/site-packages (0.12.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: setproctitle in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (1.5.8)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: pathtools in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/abdelrahman/.local/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abdelrahman/.local/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/abdelrahman/.local/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/abdelrahman/.local/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: dataclasses<0.9,>=0.8 in /home/abdelrahman/.local/lib/python3.6/site-packages (from yaspin>=1.0.0->wandb) (0.8)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /home/abdelrahman/.local/lib/python3.6/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/abdelrahman/.local/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2880b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshefa1\u001b[0m (use `wandb login --relogin` to force relogin)\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
