{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94aae934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install wandb\n",
    "# !pip install tashaphyne\n",
    "# !pip install emojis\n",
    "\n",
    "from read_write_qrcd import *\n",
    "from assets import *\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import sentencepiece\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from data_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7455fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 710 records from datasets/qrcd_v1.1_train.jsonl\n",
      "pq_id: 2:8-16_364\n",
      "pq_id: 2:174-176_364\n",
      "pq_id: 14:47-52_364\n",
      "pq_id: 17:12-17_364\n",
      "pq_id: 18:29-31_364\n",
      "pq_id: 27:89-93_364\n",
      "pq_id: 30:38-41_364\n",
      "pq_id: 39:38-41_364\n",
      "pq_id: 91:1-10_364\n",
      "pq_id: 2:8-16_378\n",
      "pq_id: 2:174-176_378\n",
      "pq_id: 2:277-281_378\n",
      "pq_id: 2:284-286_378\n",
      "pq_id: 5:48-50_378\n",
      "pq_id: 6:27-32_378\n",
      "pq_id: 6:100-105_378\n",
      "pq_id: 6:148-150_378\n",
      "pq_id: 14:47-52_378\n",
      "pq_id: 16:35-40_378\n",
      "pq_id: 17:12-17_378\n",
      "pq_id: 18:29-31_378\n",
      "pq_id: 27:89-93_378\n",
      "pq_id: 30:38-41_378\n",
      "pq_id: 35:15-18_378\n",
      "pq_id: 39:38-41_378\n",
      "pq_id: 41:45-48_378\n",
      "pq_id: 42:27-31_378\n",
      "pq_id: 45:12-15_378\n",
      "pq_id: 47:1-6_378\n",
      "pq_id: 67:1-5_378\n",
      "pq_id: 73:15-19_378\n",
      "pq_id: 74:32-48_378\n",
      "pq_id: 76:23-31_378\n",
      "pq_id: 78:38-40_378\n",
      "pq_id: 81:15-29_378\n",
      "pq_id: 91:1-10_378\n",
      "pq_id: 2:17-20_343\n",
      "pq_id: 10:3-6_343\n",
      "pq_id: 24:35-38_343\n",
      "pq_id: 2:25-25_143\n",
      "pq_id: 55:46-78_143\n",
      "pq_id: 56:11-26_143\n",
      "pq_id: 56:27-40_143\n",
      "pq_id: 2:26-27_231\n",
      "pq_id: 2:49-52_231\n",
      "pq_id: 2:53-57_231\n",
      "pq_id: 2:63-66_231\n",
      "pq_id: 2:67-73_231\n",
      "pq_id: 2:91-93_231\n",
      "pq_id: 2:172-173_231\n",
      "pq_id: 2:259-259_231\n",
      "pq_id: 2:260-260_231\n",
      "pq_id: 3:14-17_231\n",
      "pq_id: 4:153-161_231\n",
      "pq_id: 5:3-3_231\n",
      "pq_id: 5:4-5_231\n",
      "pq_id: 5:27-31_231\n",
      "pq_id: 5:59-60_231\n",
      "pq_id: 6:36-41_231\n",
      "pq_id: 6:143-144_231\n",
      "pq_id: 6:145-147_231\n",
      "pq_id: 7:73-79_231\n",
      "pq_id: 7:103-108_231\n",
      "pq_id: 7:130-136_231\n",
      "pq_id: 7:150-153_231\n",
      "pq_id: 7:159-162_231\n",
      "pq_id: 7:163-166_231\n",
      "pq_id: 7:175-178_231\n",
      "pq_id: 8:55-61_231\n",
      "pq_id: 11:61-68_231\n",
      "pq_id: 11:69-76_231\n",
      "pq_id: 12:11-15_231\n",
      "pq_id: 12:16-18_231\n",
      "pq_id: 12:36-42_231\n",
      "pq_id: 12:43-49_231\n",
      "pq_id: 12:63-66_231\n",
      "pq_id: 12:69-76_231\n",
      "pq_id: 16:1-9_231\n",
      "pq_id: 16:65-69_231\n",
      "pq_id: 16:77-79_231\n",
      "pq_id: 16:114-119_231\n",
      "pq_id: 17:59-60_231\n",
      "pq_id: 18:17-20_231\n",
      "pq_id: 18:21-22_231\n",
      "pq_id: 18:60-77_231\n",
      "pq_id: 20:17-24_231\n",
      "pq_id: 20:80-82_231\n",
      "pq_id: 21:78-82_231\n",
      "pq_id: 21:87-88_231\n",
      "pq_id: 22:30-37_231\n",
      "pq_id: 22:73-76_231\n",
      "pq_id: 24:41-45_231\n",
      "pq_id: 26:23-40_231\n",
      "pq_id: 26:141-159_231\n",
      "pq_id: 27:15-22_231\n",
      "pq_id: 27:20-28_231\n",
      "pq_id: 27:82-88_231\n",
      "pq_id: 29:41-43_231\n",
      "pq_id: 31:12-19_231\n",
      "pq_id: 34:10-13_231\n",
      "pq_id: 34:14-14_231\n",
      "pq_id: 37:139-148_231\n",
      "pq_id: 38:12-20_231\n",
      "pq_id: 51:24-37_231\n",
      "pq_id: 54:1-8_231\n",
      "pq_id: 54:23-32_231\n",
      "pq_id: 56:11-26_231\n",
      "pq_id: 59:6-7_231\n",
      "pq_id: 62:5-8_231\n",
      "pq_id: 67:16-23_231\n",
      "pq_id: 68:48-52_231\n",
      "pq_id: 74:49-56_231\n",
      "pq_id: 77:29-40_231\n",
      "pq_id: 88:17-26_231\n",
      "pq_id: 91:1-15_231\n",
      "pq_id: 101:1-11_231\n",
      "pq_id: 105:1-5_231\n",
      "pq_id: 2:30-33_164\n",
      "pq_id: 3:33-41_164\n",
      "pq_id: 7:142-143_164\n",
      "pq_id: 7:204-206_164\n",
      "pq_id: 10:7-10_164\n",
      "pq_id: 13:12-15_164\n",
      "pq_id: 17:40-44_164\n",
      "pq_id: 17:105-111_164\n",
      "pq_id: 21:16-20_164\n",
      "pq_id: 21:78-82_164\n",
      "pq_id: 24:35-38_164\n",
      "pq_id: 24:41-45_164\n",
      "pq_id: 32:15-17_164\n",
      "pq_id: 37:139-148_164\n",
      "pq_id: 37:158-169_164\n",
      "pq_id: 38:12-20_164\n",
      "pq_id: 39:71-75_164\n",
      "pq_id: 40:7-9_164\n",
      "pq_id: 41:37-39_164\n",
      "pq_id: 42:1-6_164\n",
      "pq_id: 57:1-6_164\n",
      "pq_id: 59:1-5_164\n",
      "pq_id: 59:21-24_164\n",
      "pq_id: 61:1-4_164\n",
      "pq_id: 62:1-4_164\n",
      "pq_id: 64:1-4_164\n",
      "pq_id: 2:49-52_413\n",
      "pq_id: 7:138-141_413\n",
      "pq_id: 14:5-8_413\n",
      "pq_id: 2:60-62_220\n",
      "pq_id: 10:87-89_220\n",
      "pq_id: 12:19-22_220\n",
      "pq_id: 12:97-101_220\n",
      "pq_id: 2:63-66_240\n",
      "pq_id: 7:163-166_240\n",
      "pq_id: 2:63-66_248\n",
      "pq_id: 4:153-161_248\n",
      "pq_id: 7:163-166_248\n",
      "pq_id: 2:74-75_407\n",
      "pq_id: 2:78-79_407\n",
      "pq_id: 3:78-80_407\n",
      "pq_id: 4:44-46_407\n",
      "pq_id: 5:12-13_407\n",
      "pq_id: 5:41-43_407\n",
      "pq_id: 6:91-92_407\n",
      "pq_id: 9:30-31_407\n",
      "pq_id: 2:102-103_232\n",
      "pq_id: 3:95-97_232\n",
      "pq_id: 5:20-26_232\n",
      "pq_id: 9:69-70_232\n",
      "pq_id: 9:101-106_232\n",
      "pq_id: 9:117-121_232\n",
      "pq_id: 11:84-94_232\n",
      "pq_id: 11:89-95_232\n",
      "pq_id: 20:38-48_232\n",
      "pq_id: 22:42-46_232\n",
      "pq_id: 23:17-22_232\n",
      "pq_id: 27:20-28_232\n",
      "pq_id: 28:22-28_232\n",
      "pq_id: 28:44-46_232\n",
      "pq_id: 29:36-40_232\n",
      "pq_id: 33:9-17_232\n",
      "pq_id: 33:60-62_232\n",
      "pq_id: 34:15-19_232\n",
      "pq_id: 48:24-26_232\n",
      "pq_id: 63:5-8_232\n",
      "pq_id: 89:1-14_232\n",
      "pq_id: 2:124-129_352\n",
      "pq_id: 2:130-134_352\n",
      "pq_id: 2:135-138_352\n",
      "pq_id: 3:62-68_352\n",
      "pq_id: 3:84-85_352\n",
      "pq_id: 10:71-73_352\n",
      "pq_id: 12:97-101_352\n",
      "pq_id: 27:38-44_352\n",
      "pq_id: 2:130-134_419\n",
      "pq_id: 2:135-138_419\n",
      "pq_id: 2:177-177_419\n",
      "pq_id: 2:213-214_419\n",
      "pq_id: 2:253-254_419\n",
      "pq_id: 2:284-286_419\n",
      "pq_id: 3:18-22_419\n",
      "pq_id: 3:33-41_419\n",
      "pq_id: 3:84-85_419\n",
      "pq_id: 4:66-70_419\n",
      "pq_id: 4:135-136_419\n",
      "pq_id: 4:150-152_419\n",
      "pq_id: 6:8-11_419\n",
      "pq_id: 6:84-90_419\n",
      "pq_id: 11:69-76_419\n",
      "pq_id: 13:30-34_419\n",
      "pq_id: 17:53-56_419\n",
      "pq_id: 19:41-51_419\n",
      "pq_id: 19:51-58_419\n",
      "pq_id: 21:36-41_419\n",
      "pq_id: 21:69-73_419\n",
      "pq_id: 33:7-9_419\n",
      "pq_id: 37:75-82_419\n",
      "pq_id: 38:30-40_419\n",
      "pq_id: 38:41-48_419\n",
      "pq_id: 40:47-52_419\n",
      "pq_id: 58:14-21_419\n",
      "pq_id: 2:151-153_414\n",
      "pq_id: 3:30-32_414\n",
      "pq_id: 3:130-136_414\n",
      "pq_id: 3:164-165_414\n",
      "pq_id: 4:12-14_414\n",
      "pq_id: 4:58-59_414\n",
      "pq_id: 4:64-65_414\n",
      "pq_id: 4:66-70_414\n",
      "pq_id: 4:80-84_414\n",
      "pq_id: 5:90-93_414\n",
      "pq_id: 5:103-105_414\n",
      "pq_id: 7:157-158_414\n",
      "pq_id: 8:20-26_414\n",
      "pq_id: 9:71-72_414\n",
      "pq_id: 24:46-54_414\n",
      "pq_id: 24:55-57_414\n",
      "pq_id: 24:62-64_414\n",
      "pq_id: 33:21-24_414\n",
      "pq_id: 33:36-40_414\n",
      "pq_id: 33:69-71_414\n",
      "pq_id: 47:33-38_414\n",
      "pq_id: 48:16-17_414\n",
      "pq_id: 58:12-13_414\n",
      "pq_id: 59:6-7_414\n",
      "pq_id: 64:11-13_414\n",
      "pq_id: 2:159-162_310\n",
      "pq_id: 3:86-91_310\n",
      "pq_id: 4:15-16_310\n",
      "pq_id: 4:17-18_310\n",
      "pq_id: 4:92-93_310\n",
      "pq_id: 4:144-147_310\n",
      "pq_id: 5:38-40_310\n",
      "pq_id: 6:53-58_310\n",
      "pq_id: 7:150-153_310\n",
      "pq_id: 9:1-6_310\n",
      "pq_id: 9:7-11_310\n",
      "pq_id: 11:1-5_310\n",
      "pq_id: 11:50-60_310\n",
      "pq_id: 16:114-119_310\n",
      "pq_id: 19:59-65_310\n",
      "pq_id: 20:80-82_310\n",
      "pq_id: 24:2-5_310\n",
      "pq_id: 25:63-77_310\n",
      "pq_id: 28:60-67_310\n",
      "pq_id: 58:12-13_310\n",
      "pq_id: 66:8-9_310\n",
      "pq_id: 2:177-177_325\n",
      "pq_id: 2:189-190_325\n",
      "pq_id: 3:92-94_325\n",
      "pq_id: 2:177-177_326\n",
      "pq_id: 2:189-190_326\n",
      "pq_id: 3:92-94_326\n",
      "pq_id: 19:12-15_326\n",
      "pq_id: 19:16-34_326\n",
      "pq_id: 58:7-10_326\n",
      "pq_id: 60:8-9_326\n",
      "pq_id: 76:5-11_326\n",
      "pq_id: 2:178-179_140\n",
      "pq_id: 4:92-93_140\n",
      "pq_id: 25:63-77_140\n",
      "pq_id: 2:190-194_396\n",
      "pq_id: 2:216-218_396\n",
      "pq_id: 2:196-203_311\n",
      "pq_id: 3:159-160_311\n",
      "pq_id: 14:24-27_311\n",
      "pq_id: 20:38-48_311\n",
      "pq_id: 22:23-24_311\n",
      "pq_id: 33:69-71_311\n",
      "pq_id: 35:9-11_311\n",
      "pq_id: 39:11-20_311\n",
      "pq_id: 2:216-218_303\n",
      "pq_id: 3:121-129_303\n",
      "pq_id: 3:166-168_303\n",
      "pq_id: 4:77-79_303\n",
      "pq_id: 5:35-37_303\n",
      "pq_id: 5:54-56_303\n",
      "pq_id: 8:62-66_303\n",
      "pq_id: 8:72-75_303\n",
      "pq_id: 9:19-22_303\n",
      "pq_id: 9:42-49_303\n",
      "pq_id: 9:73-74_303\n",
      "pq_id: 9:86-89_303\n",
      "pq_id: 49:14-18_303\n",
      "pq_id: 60:1-3_303\n",
      "pq_id: 61:10-14_303\n",
      "pq_id: 66:8-9_303\n",
      "pq_id: 2:216-218_408\n",
      "pq_id: 3:84-85_408\n",
      "pq_id: 3:86-91_408\n",
      "pq_id: 9:73-74_408\n",
      "pq_id: 47:25-29_408\n",
      "pq_id: 2:228-230_123\n",
      "pq_id: 33:49-52_123\n",
      "pq_id: 65:4-7_123\n",
      "pq_id: 2:228-230_424\n",
      "pq_id: 2:231-232_424\n",
      "pq_id: 2:234-237_424\n",
      "pq_id: 2:240-242_424\n",
      "pq_id: 3:190-195_424\n",
      "pq_id: 4:7-10_424\n",
      "pq_id: 4:19-21_424\n",
      "pq_id: 4:25-25_424\n",
      "pq_id: 4:32-33_424\n",
      "pq_id: 4:122-126_424\n",
      "pq_id: 4:127-127_424\n",
      "pq_id: 16:94-97_424\n",
      "pq_id: 33:35-35_424\n",
      "pq_id: 33:56-59_424\n",
      "pq_id: 40:38-46_424\n",
      "pq_id: 58:1-4_424\n",
      "pq_id: 60:12-13_424\n",
      "pq_id: 65:1-3_424\n",
      "pq_id: 65:4-7_424\n",
      "pq_id: 2:234-237_314\n",
      "pq_id: 3:133-136_314\n",
      "pq_id: 3:145-148_314\n",
      "pq_id: 3:169-175_314\n",
      "pq_id: 4:122-126_314\n",
      "pq_id: 4:128-130_314\n",
      "pq_id: 5:12-13_314\n",
      "pq_id: 5:82-86_314\n",
      "pq_id: 5:90-93_314\n",
      "pq_id: 7:54-56_314\n",
      "pq_id: 9:90-96_314\n",
      "pq_id: 9:117-121_314\n",
      "pq_id: 10:26-27_314\n",
      "pq_id: 11:112-115_314\n",
      "pq_id: 12:88-92_314\n",
      "pq_id: 16:30-32_314\n",
      "pq_id: 16:125-128_314\n",
      "pq_id: 18:29-31_314\n",
      "pq_id: 22:30-37_314\n",
      "pq_id: 28:76-81_314\n",
      "pq_id: 29:61-69_314\n",
      "pq_id: 33:28-31_314\n",
      "pq_id: 37:75-82_314\n",
      "pq_id: 37:99-113_314\n",
      "pq_id: 37:114-122_314\n",
      "pq_id: 37:123-132_314\n",
      "pq_id: 39:9-10_314\n",
      "pq_id: 39:32-37_314\n",
      "pq_id: 41:33-36_314\n",
      "pq_id: 53:31-41_314\n",
      "pq_id: 2:249-252_216\n",
      "pq_id: 4:163-166_216\n",
      "pq_id: 17:53-56_216\n",
      "pq_id: 21:78-82_216\n",
      "pq_id: 27:15-22_216\n",
      "pq_id: 34:10-13_216\n",
      "pq_id: 38:12-20_216\n",
      "pq_id: 38:21-26_216\n",
      "pq_id: 38:30-40_216\n",
      "pq_id: 2:249-252_258\n",
      "pq_id: 2:256-257_393\n",
      "pq_id: 18:29-31_393\n",
      "pq_id: 27:89-93_393\n",
      "pq_id: 39:38-41_393\n",
      "pq_id: 74:49-56_393\n",
      "pq_id: 76:23-31_393\n",
      "pq_id: 78:38-40_393\n",
      "pq_id: 109:1-6_393\n",
      "pq_id: 2:272-274_330\n",
      "pq_id: 2:277-281_330\n",
      "pq_id: 2:284-286_330\n",
      "pq_id: 3:176-180_330\n",
      "pq_id: 5:48-50_330\n",
      "pq_id: 5:54-56_330\n",
      "pq_id: 6:27-32_330\n",
      "pq_id: 6:100-105_330\n",
      "pq_id: 6:164-165_330\n",
      "pq_id: 11:6-7_330\n",
      "pq_id: 12:50-57_330\n",
      "pq_id: 14:47-52_330\n",
      "pq_id: 17:12-17_330\n",
      "pq_id: 18:29-31_330\n",
      "pq_id: 27:38-44_330\n",
      "pq_id: 27:89-93_330\n",
      "pq_id: 30:38-41_330\n",
      "pq_id: 31:12-19_330\n",
      "pq_id: 33:72-73_330\n",
      "pq_id: 35:15-18_330\n",
      "pq_id: 35:29-35_330\n",
      "pq_id: 39:38-41_330\n",
      "pq_id: 41:45-48_330\n",
      "pq_id: 42:27-31_330\n",
      "pq_id: 43:15-25_330\n",
      "pq_id: 45:12-15_330\n",
      "pq_id: 47:1-6_330\n",
      "pq_id: 67:1-5_330\n",
      "pq_id: 73:15-19_330\n",
      "pq_id: 74:32-48_330\n",
      "pq_id: 74:49-56_330\n",
      "pq_id: 76:23-31_330\n",
      "pq_id: 78:38-40_330\n",
      "pq_id: 79:34-41_330\n",
      "pq_id: 81:15-29_330\n",
      "pq_id: 87:6-19_330\n",
      "pq_id: 90:5-10_330\n",
      "pq_id: 91:1-10_330\n",
      "pq_id: 2:275-276_116\n",
      "pq_id: 2:277-281_116\n",
      "pq_id: 2:282-283_412\n",
      "pq_id: 3:33-41_106\n",
      "pq_id: 3:33-41_137\n",
      "pq_id: 19:1-11_137\n",
      "pq_id: 21:89-90_137\n",
      "pq_id: 3:33-41_122\n",
      "pq_id: 19:1-11_122\n",
      "pq_id: 3:45-51_155\n",
      "pq_id: 5:44-47_155\n",
      "pq_id: 5:110-113_155\n",
      "pq_id: 57:25-27_155\n",
      "pq_id: 3:45-51_160\n",
      "pq_id: 4:153-161_160\n",
      "pq_id: 4:171-172_160\n",
      "pq_id: 5:15-17_160\n",
      "pq_id: 5:74-77_160\n",
      "pq_id: 3:45-51_219\n",
      "pq_id: 3:49-53_219\n",
      "pq_id: 5:72-73_219\n",
      "pq_id: 5:78-81_219\n",
      "pq_id: 5:112-115_219\n",
      "pq_id: 61:6-9_219\n",
      "pq_id: 61:10-14_219\n",
      "pq_id: 3:49-53_212\n",
      "pq_id: 5:110-113_212\n",
      "pq_id: 61:10-14_212\n",
      "pq_id: 3:49-53_266\n",
      "pq_id: 9:100-102_266\n",
      "pq_id: 9:117-121_266\n",
      "pq_id: 61:10-14_266\n",
      "pq_id: 3:54-58_404\n",
      "pq_id: 4:153-161_404\n",
      "pq_id: 5:116-120_404\n",
      "pq_id: 3:95-97_422\n",
      "pq_id: 5:20-26_422\n",
      "pq_id: 17:1-1_422\n",
      "pq_id: 20:9-16_422\n",
      "pq_id: 79:15-26_422\n",
      "pq_id: 3:121-129_265\n",
      "pq_id: 3:137-144_357\n",
      "pq_id: 4:95-96_357\n",
      "pq_id: 5:35-37_357\n",
      "pq_id: 5:54-56_357\n",
      "pq_id: 8:72-75_357\n",
      "pq_id: 9:19-22_357\n",
      "pq_id: 9:38-41_357\n",
      "pq_id: 9:86-89_357\n",
      "pq_id: 29:61-69_357\n",
      "pq_id: 3:187-189_402\n",
      "pq_id: 9:30-31_402\n",
      "pq_id: 4:2-6_136\n",
      "pq_id: 4:36-40_401\n",
      "pq_id: 10:61-61_401\n",
      "pq_id: 34:3-5_401\n",
      "pq_id: 34:20-23_401\n",
      "pq_id: 99:1-8_401\n",
      "pq_id: 4:80-84_421\n",
      "pq_id: 10:37-44_421\n",
      "pq_id: 15:1-9_421\n",
      "pq_id: 17:105-111_421\n",
      "pq_id: 39:27-31_421\n",
      "pq_id: 41:40-44_421\n",
      "pq_id: 43:1-8_421\n",
      "pq_id: 75:16-25_421\n",
      "pq_id: 85:12-22_421\n",
      "pq_id: 4:95-96_356\n",
      "pq_id: 8:72-75_356\n",
      "pq_id: 9:19-22_356\n",
      "pq_id: 9:38-41_356\n",
      "pq_id: 9:42-49_356\n",
      "pq_id: 9:73-74_356\n",
      "pq_id: 9:81-85_356\n",
      "pq_id: 9:86-89_356\n",
      "pq_id: 47:1-6_356\n",
      "pq_id: 49:14-18_356\n",
      "pq_id: 61:10-14_356\n",
      "pq_id: 66:8-9_356\n",
      "pq_id: 4:101-103_362\n",
      "pq_id: 7:80-84_362\n",
      "pq_id: 8:30-37_362\n",
      "pq_id: 11:77-83_362\n",
      "pq_id: 15:61-77_362\n",
      "pq_id: 25:35-40_362\n",
      "pq_id: 26:160-175_362\n",
      "pq_id: 27:54-58_362\n",
      "pq_id: 46:21-25_362\n",
      "pq_id: 4:114-115_342\n",
      "pq_id: 7:204-206_342\n",
      "pq_id: 22:25-29_342\n",
      "pq_id: 31:12-19_342\n",
      "pq_id: 43:74-80_342\n",
      "pq_id: 49:1-5_342\n",
      "pq_id: 58:7-10_342\n",
      "pq_id: 68:48-52_342\n",
      "pq_id: 5:4-5_410\n",
      "pq_id: 5:82-86_410\n",
      "pq_id: 60:8-9_410\n",
      "pq_id: 5:6-6_380\n",
      "pq_id: 5:6-6_379\n",
      "pq_id: 5:27-31_205\n",
      "pq_id: 5:38-40_118\n",
      "pq_id: 5:51-53_426\n",
      "pq_id: 5:57-58_426\n",
      "pq_id: 5:82-86_341\n",
      "pq_id: 7:204-206_341\n",
      "pq_id: 39:11-20_341\n",
      "pq_id: 50:36-45_341\n",
      "pq_id: 62:9-11_341\n",
      "pq_id: 5:89-89_147\n",
      "pq_id: 5:101-102_385\n",
      "pq_id: 5:106-108_202\n",
      "pq_id: 6:80-86_204\n",
      "pq_id: 11:69-76_204\n",
      "pq_id: 14:35-41_204\n",
      "pq_id: 19:41-51_204\n",
      "pq_id: 37:99-113_204\n",
      "pq_id: 6:84-90_371\n",
      "pq_id: 40:34-35_371\n",
      "pq_id: 6:125-127_405\n",
      "pq_id: 7:54-56_152\n",
      "pq_id: 10:3-6_152\n",
      "pq_id: 11:6-7_152\n",
      "pq_id: 25:56-62_152\n",
      "pq_id: 32:4-9_152\n",
      "pq_id: 41:9-12_152\n",
      "pq_id: 50:36-45_152\n",
      "pq_id: 57:1-6_152\n",
      "pq_id: 7:73-79_115\n",
      "pq_id: 11:61-68_115\n",
      "pq_id: 17:59-60_115\n",
      "pq_id: 54:23-32_115\n",
      "pq_id: 91:1-15_115\n",
      "pq_id: 7:73-79_225\n",
      "pq_id: 11:61-68_225\n",
      "pq_id: 17:59-60_225\n",
      "pq_id: 54:23-32_225\n",
      "pq_id: 7:80-84_397\n",
      "pq_id: 26:160-175_397\n",
      "pq_id: 27:54-58_397\n",
      "pq_id: 29:28-35_397\n",
      "pq_id: 54:33-42_397\n",
      "pq_id: 7:85-93_101\n",
      "pq_id: 11:84-94_101\n",
      "pq_id: 26:176-191_101\n",
      "pq_id: 29:36-40_101\n",
      "pq_id: 7:85-93_245\n",
      "pq_id: 11:84-94_245\n",
      "pq_id: 26:176-191_245\n",
      "pq_id: 29:36-40_245\n",
      "pq_id: 7:103-108_102\n",
      "pq_id: 10:74-78_102\n",
      "pq_id: 11:96-99_102\n",
      "pq_id: 17:2-8_102\n",
      "pq_id: 17:101-104_102\n",
      "pq_id: 20:17-24_102\n",
      "pq_id: 20:38-48_102\n",
      "pq_id: 23:45-50_102\n",
      "pq_id: 26:10-22_102\n",
      "pq_id: 28:76-81_102\n",
      "pq_id: 29:38-40_102\n",
      "pq_id: 32:23-27_102\n",
      "pq_id: 40:23-27_102\n",
      "pq_id: 40:53-58_102\n",
      "pq_id: 43:46-56_102\n",
      "pq_id: 51:38-46_102\n",
      "pq_id: 79:15-26_102\n",
      "pq_id: 7:142-143_108\n",
      "pq_id: 19:51-58_108\n",
      "pq_id: 20:25-37_108\n",
      "pq_id: 20:86-94_108\n",
      "pq_id: 23:45-50_108\n",
      "pq_id: 25:35-40_108\n",
      "pq_id: 28:33-35_108\n",
      "pq_id: 9:12-16_350\n",
      "pq_id: 10:57-60_350\n",
      "pq_id: 16:65-69_350\n",
      "pq_id: 17:78-82_350\n",
      "pq_id: 26:69-89_350\n",
      "pq_id: 41:40-44_350\n",
      "pq_id: 9:36-37_125\n",
      "pq_id: 10:15-17_411\n",
      "pq_id: 15:1-9_411\n",
      "pq_id: 41:40-44_411\n",
      "pq_id: 75:16-25_411\n",
      "pq_id: 10:24-25_308\n",
      "pq_id: 18:45-46_308\n",
      "pq_id: 57:18-21_308\n",
      "pq_id: 10:87-89_227\n",
      "pq_id: 43:46-56_227\n",
      "pq_id: 11:41-44_129\n",
      "pq_id: 11:41-44_207\n",
      "pq_id: 11:45-48_207\n",
      "pq_id: 11:50-60_337\n",
      "pq_id: 55:37-45_337\n",
      "pq_id: 96:9-19_337\n",
      "pq_id: 12:1-3_159\n",
      "pq_id: 20:102-113_159\n",
      "pq_id: 39:27-31_159\n",
      "pq_id: 41:1-8_159\n",
      "pq_id: 41:40-44_159\n",
      "pq_id: 42:7-12_159\n",
      "pq_id: 43:1-8_159\n",
      "pq_id: 12:4-6_144\n",
      "pq_id: 12:67-69_144\n",
      "pq_id: 12:7-10_224\n",
      "pq_id: 12:23-33_128\n",
      "pq_id: 12:97-101_128\n",
      "pq_id: 12:39-54_237\n",
      "pq_id: 12:43-49_354\n",
      "pq_id: 29:14-18_354\n",
      "pq_id: 13:18-24_360\n",
      "pq_id: 40:7-9_360\n",
      "pq_id: 52:17-28_360\n",
      "pq_id: 16:84-89_389\n",
      "pq_id: 17:88-89_389\n",
      "pq_id: 18:54-56_389\n",
      "pq_id: 30:55-60_389\n",
      "pq_id: 39:27-31_389\n",
      "pq_id: 17:2-8_365\n",
      "pq_id: 17:101-104_365\n",
      "pq_id: 18:60-82_253\n",
      "pq_id: 18:83-98_209\n",
      "pq_id: 18:83-98_328\n",
      "pq_id: 21:96-100_328\n",
      "pq_id: 22:1-2_328\n",
      "pq_id: 54:1-8_328\n",
      "pq_id: 75:1-15_328\n",
      "pq_id: 19:12-15_327\n",
      "pq_id: 19:16-34_327\n",
      "pq_id: 76:5-11_327\n",
      "pq_id: 82:13-19_327\n",
      "pq_id: 83:18-28_327\n",
      "pq_id: 19:16-34_363\n",
      "pq_id: 19:51-58_361\n",
      "pq_id: 22:52-57_361\n",
      "pq_id: 33:36-40_361\n",
      "pq_id: 19:77-87_367\n",
      "pq_id: 20:102-113_367\n",
      "pq_id: 21:25-29_367\n",
      "pq_id: 40:7-9_367\n",
      "pq_id: 43:81-89_367\n",
      "pq_id: 53:19-30_367\n",
      "pq_id: 21:30-35_347\n",
      "pq_id: 22:58-66_347\n",
      "pq_id: 31:29-32_347\n",
      "pq_id: 35:12-14_347\n",
      "pq_id: 36:33-47_347\n",
      "pq_id: 39:1-5_347\n",
      "pq_id: 57:1-6_347\n",
      "pq_id: 21:78-82_120\n",
      "pq_id: 27:15-22_120\n",
      "pq_id: 21:83-86_105\n",
      "pq_id: 38:41-44_105\n",
      "pq_id: 21:87-88_249\n",
      "pq_id: 37:139-148_249\n",
      "pq_id: 68:48-52_249\n",
      "pq_id: 24:2-5_368\n",
      "pq_id: 24:6-10_368\n",
      "pq_id: 24:23-26_368\n",
      "pq_id: 24:30-31_301\n",
      "pq_id: 33:53-55_301\n",
      "pq_id: 33:56-59_301\n",
      "pq_id: 24:41-45_409\n",
      "pq_id: 27:15-22_409\n",
      "pq_id: 27:20-28_409\n",
      "pq_id: 27:82-88_409\n",
      "pq_id: 27:15-22_243\n",
      "pq_id: 28:76-81_114\n",
      "pq_id: 28:76-81_145\n",
      "pq_id: 29:38-40_145\n",
      "pq_id: 40:23-27_145\n",
      "pq_id: 29:46-47_427\n",
      "pq_id: 33:4-6_395\n",
      "pq_id: 37:62-74_119\n",
      "pq_id: 44:40-50_119\n",
      "pq_id: 56:41-56_119\n",
      "pq_id: 37:62-74_127\n",
      "pq_id: 44:40-50_127\n",
      "pq_id: 56:41-56_127\n",
      "pq_id: 37:139-148_138\n",
      "pq_id: 46:21-25_235\n",
      "pq_id: 89:1-14_235\n",
      "pq_id: 46:29-32_398\n",
      "pq_id: 72:1-7_398\n",
      "pq_id: 46:29-32_399\n",
      "pq_id: 72:1-7_399\n",
      "pq_id: 58:1-4_146\n",
      "pq_id: 83:1-17_141\n",
      "pq_id: 97:1-5_153\n",
      "pq_id: 101:1-11_109\n",
      "Collected 710 Object from datasets/qrcd_v1.1_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "passage_question_objects = read_JSONL_file('datasets/qrcd_v1.1_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab663cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "df_file = json_to_dataframe(passage_question_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9865d18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2:8-16_364</td>\n",
       "      <td>ومن الناس من يقول امنا بالله وباليوم الاخر وما...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['اولئك الذين اشتروا الضلاله بالهدي']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2:174-176_364</td>\n",
       "      <td>ان الذين يكتمون ما انزل الله من الكتاب ويشترون...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['اولئك الذين اشتروا الضلاله بالهدي و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14:47-52_364</td>\n",
       "      <td>فلا تحسبن الله مخلف وعده رسله ان الله عزيز ذو ...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['ليجزي الله كل نفس ما كسبت'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17:12-17_364</td>\n",
       "      <td>وجعلنا الليل والنهار ايتين فمحونا ايه الليل وج...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['كل انسان الزمناه طائره في عنقه', 'م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18:29-31_364</td>\n",
       "      <td>وقل الحق من ربكم فمن شاء فليؤمن ومن شاء فليكفر...</td>\n",
       "      <td>لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...</td>\n",
       "      <td>{'text': ['من شاء فليؤمن ومن شاء فليكفر'], 'an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            context  \\\n",
       "0     2:8-16_364  ومن الناس من يقول امنا بالله وباليوم الاخر وما...   \n",
       "1  2:174-176_364  ان الذين يكتمون ما انزل الله من الكتاب ويشترون...   \n",
       "2   14:47-52_364  فلا تحسبن الله مخلف وعده رسله ان الله عزيز ذو ...   \n",
       "3   17:12-17_364  وجعلنا الليل والنهار ايتين فمحونا ايه الليل وج...   \n",
       "4   18:29-31_364  وقل الحق من ربكم فمن شاء فليؤمن ومن شاء فليكفر...   \n",
       "\n",
       "                                            question  \\\n",
       "0  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "1  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "2  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "3  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "4  لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['اولئك الذين اشتروا الضلاله بالهدي']...  \n",
       "1  {'text': ['اولئك الذين اشتروا الضلاله بالهدي و...  \n",
       "2  {'text': ['ليجزي الله كل نفس ما كسبت'], 'answe...  \n",
       "3  {'text': ['كل انسان الزمناه طائره في عنقه', 'م...  \n",
       "4  {'text': ['من شاء فليؤمن ومن شاء فليكفر'], 'an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72ca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517cdc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d2694f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2:8-16_364',\n",
       " 'context': 'ومن الناس من يقول امنا بالله وباليوم الاخر وما هم بمؤمنين . يخادعون الله والذين امنوا وما يخدعون الا انفسهم وما يشعرون . في قلوبهم مرض فزادهم الله مرضا ولهم عذاب اليم بما كانوا يكذبون . واذا قيل لهم لا تفسدوا في الارض قالوا انما نحن مصلحون . الا انهم هم المفسدون ولكن لا يشعرون . واذا قيل لهم امنوا كما امن الناس قالوا انؤمن كما امن السفهاء الا انهم هم السفهاء ولكن لا يعلمون . واذا لقوا الذين امنوا قالوا امنا واذا خلوا الي شياطينهم قالوا انا معكم انما نحن مستهزئون . الله يستهزئ بهم ويمدهم في طغيانهم يعمهون . اولئك الذين اشتروا الضلاله بالهدي فما ربحت تجارتهم وما كانوا مهتدين . ',\n",
       " 'question': 'لماذا سيحاسب ويعذب الضال يوم القيامه ان كان من يضلل الله فما له من هاد كما ورد من قوله تعالي في ايه 23 و ايه 36 من سوره الزمر ؟ ',\n",
       " 'answers': {'answer_start': [504],\n",
       "  'text': ['اولئك الذين اشتروا الضلاله بالهدي']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61bc603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb51c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:واذكر اخا عاد اذ انذر قومه بالاحقاف وقد خلت النذر من بين يديه ومن خلفه الا تعبدوا الا الله اني اخاف عليكم عذاب يوم عظيم . قالوا اجئتنا لتافكنا عن الهتنا فاتنا بما تعدنا ان كنت من الصادقين . قال انما العلم عند الله وابلغكم ما ارسلت به ولكني اراكم قوما تجهلون . فلما راوه عارضا مستقبل اوديتهم قالوا هذا عارض ممطرنا بل هو ما استعجلتم به ريح فيها عذاب اليم . تدمر كل شيء بامر ربها فاصبحوا لا يري الا مساكنهم كذلك نجزي القوم المجرمين . \n",
      "Question:اين عاش قوم عاد ؟ \n",
      "Answer:['بالاحقاف']\n",
      "Answer Start in Text:[27]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:يا ايها الذين امنوا من يرتد منكم عن دينه فسوف ياتي الله بقوم يحبهم ويحبونه اذله علي المؤمنين اعزه علي الكافرين يجاهدون في سبيل الله ولا يخافون لومه لائم ذلك فضل الله يؤتيه من يشاء والله واسع عليم . انما وليكم الله ورسوله والذين امنوا الذين يقيمون الصلاه ويؤتون الزكاه وهم راكعون . ومن يتول الله ورسوله والذين امنوا فان حزب الله هم الغالبون . \n",
      "Question:ما هي الدلائل التي تشير بان الانسان مخير ؟ \n",
      "Answer:['يا ايها الذين امنوا من يرتد منكم عن دينه فسوف ياتي الله بقوم يحبهم ويحبونه']\n",
      "Answer Start in Text:[0]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:يا ايها الذين امنوا من يرتد منكم عن دينه فسوف ياتي الله بقوم يحبهم ويحبونه اذله علي المؤمنين اعزه علي الكافرين يجاهدون في سبيل الله ولا يخافون لومه لائم ذلك فضل الله يؤتيه من يشاء والله واسع عليم . انما وليكم الله ورسوله والذين امنوا الذين يقيمون الصلاه ويؤتون الزكاه وهم راكعون . ومن يتول الله ورسوله والذين امنوا فان حزب الله هم الغالبون . \n",
      "Question:ما هي الدلائل التي تشير بان الانسان مخير ؟ \n",
      "Answer:['يا ايها الذين امنوا من يرتد منكم عن دينه فسوف ياتي الله بقوم يحبهم ويحبونه']\n",
      "Answer Start in Text:[0]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:وهبنا لداود سليمان نعم العبد انه اواب . اذ عرض عليه بالعشي الصافنات الجياد . فقال اني احببت حب الخير عن ذكر ربي حتي توارت بالحجاب . ردوها علي فطفق مسحا بالسوق والاعناق . ولقد فتنا سليمان والقينا علي كرسيه جسدا ثم اناب . قال رب اغفر لي وهب لي ملكا لا ينبغي لاحد من بعدي انك انت الوهاب . فسخرنا له الريح تجري بامره رخاء حيث اصاب . والشياطين كل بناء وغواص . واخرين مقرنين في الاصفاد . هذا عطاؤنا فامنن او امسك بغير حساب . وان له عندنا لزلفي وحسن ماب . \n",
      "Question:هل احترم الاسلام الانبياء ؟ \n",
      "Answer:['وهبنا لداود سليمان نعم العبد انه اواب']\n",
      "Answer Start in Text:[1]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:وقالوا كونوا هودا او نصاري تهتدوا قل بل مله ابراهيم حنيفا وما كان من المشركين . قولوا امنا بالله وما انزل الينا وما انزل الي ابراهيم واسماعيل واسحاق ويعقوب والاسباط وما اوتي موسي وعيسي وما اوتي النبيون من ربهم لا نفرق بين احد منهم ونحن له مسلمون . فان امنوا بمثل ما امنتم به فقد اهتدوا وان تولوا فانما هم في شقاق فسيكفيكهم الله وهو السميع العليم . صبغه الله ومن احسن من الله صبغه ونحن له عابدون . \n",
      "Question:هل احترم الاسلام الانبياء ؟ \n",
      "Answer:['قولوا امنا بالله وما انزل الينا وما انزل الي ابراهيم واسماعيل واسحاق ويعقوب والاسباط وما اوتي موسي وعيسي وما اوتي النبيون من ربهم', 'لا نفرق بين احد منهم']\n",
      "Answer Start in Text:[79, 209]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:لقد كان لسبا في مسكنهم ايه جنتان عن يمين وشمال كلوا من رزق ربكم واشكروا له بلده طيبه ورب غفور . فاعرضوا فارسلنا عليهم سيل العرم وبدلناهم بجنتيهم جنتين ذواتي اكل خمط واثل وشيء من سدر قليل . ذلك جزيناهم بما كفروا وهل نجازي الا الكفور . وجعلنا بينهم وبين القري التي باركنا فيها قري ظاهره وقدرنا فيها السير سيروا فيها ليالي واياما امنين . فقالوا ربنا باعد بين اسفارنا وظلموا انفسهم فجعلناهم احاديث ومزقناهم كل ممزق ان في ذلك لايات لكل صبار شكور . \n",
      "Question:ما هي اسماء المدن المذكوره في القران ؟ \n",
      "Answer:['سبا']\n",
      "Answer Start in Text:[9]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:يا ايها الذين امنوا توبوا الي الله توبه نصوحا عسي ربكم ان يكفر عنكم سيئاتكم ويدخلكم جنات تجري من تحتها الانهار يوم لا يخزي الله النبي والذين امنوا معه نورهم يسعي بين ايديهم وبايمانهم يقولون ربنا اتمم لنا نورنا واغفر لنا انك علي كل شيء قدير . يا ايها النبي جاهد الكفار والمنافقين واغلظ عليهم وماواهم جهنم وبئس المصير . \n",
      "Question:ما هي انواع الجهاد ؟ \n",
      "Answer:['جاهد الكفار والمنافقين واغلظ عليهم']\n",
      "Answer Start in Text:[255]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ورسولا الي بني اسرائيل اني قد جئتكم بايه من ربكم اني اخلق لكم من الطين كهيئه الطير فانفخ فيه فيكون طيرا باذن الله وابرئ الاكمه والابرص واحي الموتي باذن الله وانبئكم بما تاكلون وما تدخرون في بيوتكم ان في ذلك لايه لكم ان كنتم مؤمنين . ومصدقا لما بين يدي من التوراه ولاحل لكم بعض الذي حرم عليكم وجئتكم بايه من ربكم فاتقوا الله واطيعون . ان الله ربي وربكم فاعبدوه هذا صراط مستقيم . فلما احس عيسي منهم الكفر قال من انصاري الي الله قال الحواريون نحن انصار الله امنا بالله واشهد بانا مسلمون . ربنا امنا بما انزلت واتبعنا الرسول فاكتبنا مع الشاهدين . \n",
      "Question:من الذي عايش سيدنا عيسي عليه السلام ؟ \n",
      "Answer:['الحواريون']\n",
      "Answer Start in Text:[428]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:يا ايها الذين امنوا توبوا الي الله توبه نصوحا عسي ربكم ان يكفر عنكم سيئاتكم ويدخلكم جنات تجري من تحتها الانهار يوم لا يخزي الله النبي والذين امنوا معه نورهم يسعي بين ايديهم وبايمانهم يقولون ربنا اتمم لنا نورنا واغفر لنا انك علي كل شيء قدير . يا ايها النبي جاهد الكفار والمنافقين واغلظ عليهم وماواهم جهنم وبئس المصير . \n",
      "Question:ما هي شروط قبول التوبه ؟ \n",
      "Answer:['توبوا الي الله توبه نصوحا']\n",
      "Answer Start in Text:[20]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:ولقد ارسلنا موسي باياتنا ان اخرج قومك من الظلمات الي النور وذكرهم بايام الله ان في ذلك لايات لكل صبار شكور . واذ قال موسي لقومه اذكروا نعمه الله عليكم اذ انجاكم من ال فرعون يسومونكم سوء العذاب ويذبحون ابناءكم ويستحيون نساءكم وفي ذلكم بلاء من ربكم عظيم . واذ تاذن ربكم لئن شكرتم لازيدنكم ولئن كفرتم ان عذابي لشديد . وقال موسي ان تكفروا انتم ومن في الارض جميعا فان الله لغني حميد . \n",
      "Question:هل يجوز سبي النساء واسترقاقهن كما تفعله بعض الجهات المنتسبه للاسلام بالباطل ؟ \n",
      "Answer:['يستحيون نساءكم وفي ذلكم بلاء من ربكم عظيم']\n",
      "Answer Start in Text:[209]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# datasets = load_dataset(\"squad\")\n",
    "def visualize(datasets, n_questions=10):\n",
    "    n = len(datasets)\n",
    "    random_questions=random.choices(list(range(n)),k=n_questions)\n",
    "    \n",
    "    for i in random_questions:\n",
    "        print(f\"Context:{datasets[i]['context']}\")\n",
    "        print(f\"Question:{datasets[i]['question']}\")\n",
    "        print(f\"Answer:{datasets[i]['answers']['text']}\")\n",
    "        print(f\"Answer Start in Text:{datasets[i]['answers']['answer_start']}\")\n",
    "        print(\"-\"*100)\n",
    "visualize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ecf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with long docs:\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" ],\n",
    "        examples[\"context\" ],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    # Looks like [0,1,2,2,2,3,4,5,5...] - Here 2nd input pair has been split in 3 parts\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    # Looks like [[(0,0),(0,3),(3,4)...] ] - Contains the actual start indices and end indices for each word in the input.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    print(\"First===================\")\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd04e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'context', 'question', 'answers']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d3624be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9689651791244ef089d30a640170631c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/771 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd6f771c3be4f6f94b273250b370cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/514M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96641b0859d4039ab0416496b512036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/390 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a73decd3ad468c9aaa4ffeafd6a461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a16334cf37444183c336d799baa841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"salti/AraElectra-base-finetuned-ARCD\"\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6053d6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e3bab6cb404a1ebd82f516bfa06b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First===================\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910dec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3346,\n",
       " 20120,\n",
       " 1006,\n",
       " 225,\n",
       " 1845,\n",
       " 2552,\n",
       " 41902,\n",
       " 759,\n",
       " 4019,\n",
       " 195,\n",
       " 338,\n",
       " 418,\n",
       " 306,\n",
       " 7641,\n",
       " 3986,\n",
       " 647,\n",
       " 3671,\n",
       " 595,\n",
       " 306,\n",
       " 9237,\n",
       " 547,\n",
       " 3663,\n",
       " 306,\n",
       " 4151,\n",
       " 55497,\n",
       " 305,\n",
       " 14071,\n",
       " 2474,\n",
       " 139,\n",
       " 14071,\n",
       " 4337,\n",
       " 306,\n",
       " 16398,\n",
       " 195,\n",
       " 33992,\n",
       " 105,\n",
       " 3,\n",
       " 623,\n",
       " 1571,\n",
       " 306,\n",
       " 1721,\n",
       " 5330,\n",
       " 181,\n",
       " 10515,\n",
       " 2609,\n",
       " 883,\n",
       " 201,\n",
       " 11356,\n",
       " 1177,\n",
       " 1891,\n",
       " 28076,\n",
       " 9133,\n",
       " 20,\n",
       " 2186,\n",
       " 47787,\n",
       " 319,\n",
       " 647,\n",
       " 6767,\n",
       " 5330,\n",
       " 330,\n",
       " 1177,\n",
       " 2186,\n",
       " 36288,\n",
       " 319,\n",
       " 335,\n",
       " 15265,\n",
       " 1177,\n",
       " 16391,\n",
       " 20,\n",
       " 305,\n",
       " 25041,\n",
       " 3524,\n",
       " 34968,\n",
       " 22890,\n",
       " 647,\n",
       " 41924,\n",
       " 28825,\n",
       " 34064,\n",
       " 11763,\n",
       " 1199,\n",
       " 2476,\n",
       " 35029,\n",
       " 319,\n",
       " 20,\n",
       " 6762,\n",
       " 10007,\n",
       " 1493,\n",
       " 391,\n",
       " 52708,\n",
       " 330,\n",
       " 305,\n",
       " 6327,\n",
       " 9466,\n",
       " 9448,\n",
       " 2006,\n",
       " 28231,\n",
       " 319,\n",
       " 20,\n",
       " 335,\n",
       " 5014,\n",
       " 1891,\n",
       " 32758,\n",
       " 5326,\n",
       " 847,\n",
       " 391,\n",
       " 16391,\n",
       " 20,\n",
       " 6762,\n",
       " 10007,\n",
       " 1493,\n",
       " 5330,\n",
       " 330,\n",
       " 547,\n",
       " 5330,\n",
       " 1571,\n",
       " 9466,\n",
       " 338,\n",
       " 11148,\n",
       " 547,\n",
       " 5330,\n",
       " 1512,\n",
       " 2180,\n",
       " 335,\n",
       " 5014,\n",
       " 1891,\n",
       " 1512,\n",
       " 2180,\n",
       " 847,\n",
       " 391,\n",
       " 20132,\n",
       " 20,\n",
       " 6762,\n",
       " 25642,\n",
       " 860,\n",
       " 5330,\n",
       " 330,\n",
       " 9466,\n",
       " 5330,\n",
       " 181,\n",
       " 6762,\n",
       " 28615,\n",
       " 181,\n",
       " 487,\n",
       " 45772,\n",
       " 6571,\n",
       " 336,\n",
       " 9466,\n",
       " 6728,\n",
       " 23288,\n",
       " 9448,\n",
       " 2006,\n",
       " 587,\n",
       " 18239,\n",
       " 4399,\n",
       " 20,\n",
       " 647,\n",
       " 885,\n",
       " 18239,\n",
       " 233,\n",
       " 3905,\n",
       " 2825,\n",
       " 5695,\n",
       " 201,\n",
       " 305,\n",
       " 44009,\n",
       " 336,\n",
       " 29283,\n",
       " 15498,\n",
       " 20,\n",
       " 23566,\n",
       " 860,\n",
       " 2537,\n",
       " 10119,\n",
       " 792,\n",
       " 25596,\n",
       " 6334,\n",
       " 458,\n",
       " 3671,\n",
       " 15762,\n",
       " 228,\n",
       " 9914,\n",
       " 1446,\n",
       " 1177,\n",
       " 2476,\n",
       " 1746,\n",
       " 30861,\n",
       " 307,\n",
       " 20,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "228a9294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3070 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3070 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 712\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 135\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/Shefa-Q-A/wandb/run-20220322_115641-2vbot4f6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shefa1/huggingface/runs/2vbot4f6\" target=\"_blank\">test-squad</a></strong> to <a href=\"https://wandb.ai/shefa1/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m default_data_collator\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     13\u001b[0m     model,\n\u001b[1;32m     14\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-squad-trained\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1403\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1406\u001b[0m ):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1984\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1987\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2016\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2015\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/electra/modeling_electra.py:1369\u001b[0m, in \u001b[0;36mElectraForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1369\u001b[0m discriminator_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melectra\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m discriminator_hidden_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1382\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/electra/modeling_electra.py:890\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 890\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:310\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape for input_ids (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or attention_mask (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mextended_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    311\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = default_data_collator\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(trainer.save_model(\"test-squad-trained\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15343513",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05768af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7c140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
