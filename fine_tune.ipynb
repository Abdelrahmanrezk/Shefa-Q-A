{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23198bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d983aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    pred = model(**inputs)\n",
    "    answer_start_scores, answer_end_scores = pred['start_logits'][0] ,pred['end_logits'][0]\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d406d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabb7935217e4461b18d6f2616f94ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2ebdf4dca84ebfb8d52b425b5d98f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /home/abdelrahman/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987a3578659f48b3ba6bfea5677dee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648b024b243e43d49238557de030313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6ee0bda1554922a69dbf72e5235263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab8479cfeb240989189363b7cd3d45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a820beaf3f24194970b94bdeaee0d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ab90329900427fa96c5b897dfe3018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /home/abdelrahman/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3321abb1844442079c03815eb78ffe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:On 18 January 2014, the interim government successfully institutionalised a more secular constitution. The president is elected to a four-year term and may serve 2 terms. The parliament may impeach the president. Under the constitution, there is a guarantee of gender equality and absolute freedom of thought. The military retains the ability to appoint the national Minister of Defence for the next 8 years. Under the constitution, political parties may not be based on \"religion, race, gender or geography\".\n",
      "Question:Who may impeach the president?\n",
      "Answer:['parliament']\n",
      "Answer Start in Text:[175]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:The Russians avoided Napoleon's objective of a decisive engagement and instead retreated deeper into Russia. A brief attempt at resistance was made at Smolensk in August; the Russians were defeated in a series of battles, and Napoleon resumed his advance. The Russians again avoided battle, although in a few cases this was only achieved because Napoleon uncharacteristically hesitated to attack when the opportunity arose. Owing to the Russian army's scorched earth tactics, the French found it increasingly difficult to forage food for themselves and their horses.\n",
      "Question:To avoid direct fighting with Napoleon, the Russians retreated into which country?\n",
      "Answer:['Russia']\n",
      "Answer Start in Text:[101]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:In 1516, William IV, Duke of Bavaria, adopted the Reinheitsgebot (purity law), perhaps the oldest food-quality regulation still in use in the 21st century, according to which the only allowed ingredients of beer are water, hops and barley-malt. Beer produced before the Industrial Revolution continued to be made and sold on a domestic scale, although by the 7th century AD, beer was also being produced and sold by European monasteries. During the Industrial Revolution, the production of beer moved from artisanal manufacture to industrial manufacture, and domestic manufacture ceased to be significant by the end of the 19th century. The development of hydrometers and thermometers changed brewing by allowing the brewer more control of the process and greater knowledge of the results.\n",
      "Question:Who was the Duke of Bavaria in 1516?\n",
      "Answer:['William IV']\n",
      "Answer Start in Text:[9]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:By 1878, because of the growing popularity of the city, one railroad line could no longer keep up with demand. Soon, the Philadelphia and Atlantic City Railway was also constructed to transport tourists to Atlantic City. At this point massive hotels like The United States and Surf House, as well as smaller rooming houses, had sprung up all over town. The United States Hotel took up a full city block between Atlantic, Pacific, Delaware, and Maryland Avenues. These hotels were not only impressive in size, but featured the most updated amenities, and were considered quite luxurious for their time.\n",
      "Question:Besides massive hotels, what else was appearing all over Atlantic City during this period?\n",
      "Answer:['rooming houses']\n",
      "Answer Start in Text:[308]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:However, all of these facets of medieval university life are considered by standard scholarship to be independent medieval European developments with no tracable Islamic influence. Generally, some reviewers have pointed out the strong inclination of Makdisi of overstating his case by simply resting on \"the accumulation of close parallels\", but all the while failing to point to convincing channels of transmission between the Muslim and Christian world. Norman Daniel points out that the Arab equivalent of the Latin disputation, the taliqa, was reserved for the ruler's court, not the madrasa, and that the actual differences between Islamic fiqh and medieval European civil law were profound. The taliqa only reached Islamic Spain, the only likely point of transmission, after the establishment of the first medieval universities. In fact, there is no Latin translation of the taliqa and, most importantly, no evidence of Latin scholars ever showing awareness of Arab influence on the Latin method of disputation, something they would have certainly found noteworthy. Rather, it was the medieval reception of the Greek Organon which set the scholastic sic et non in motion. Daniel concludes that resemblances in method had more to with the two religions having \"common problems: to reconcile the conflicting statements of their own authorities, and to safeguard the data of revelation from the impact of Greek philosophy\"; thus Christian scholasticism and similar Arab concepts should be viewed in terms of a parallel occurrence, not of the transmission of ideas from one to the other, a view shared by Hugh Kennedy.\n",
      "Question:What was Makdisi accused of doing when evaluating the parallels between European and Islamic schools?\n",
      "Answer:['overstating his case']\n",
      "Answer Start in Text:[261]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:Adult contemporary music (AC) is a style of music, ranging from 1960s vocal and 1970s soft rock music to predominantly ballad-heavy music of the present day, with varying degrees of easy listening, pop, soul, rhythm and blues, quiet storm, and rock influence. Adult contemporary is rather a continuation of the easy listening and soft rock style that became popular in the 1960s and 1970s with some adjustments that reflect the evolution of pop/rock music.\n",
      "Question:What type of adult contemporary music was popular in the 1960s?\n",
      "Answer:['vocal']\n",
      "Answer Start in Text:[70]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:Nanjing is the intersection of Yangtze River, an east-west water transport artery, and Nanjing–Beijing railway, a south-north land transport artery, hence the name “door of the east and west, throat of the south and north”. Furthermore, the west part of the Ningzhen range is in Nanjing; the Loong-like Zhong Mountain is curling in the east of the city; the tiger-like Stone Mountain is crouching in the west of the city, hence the name “the Zhong Mountain, a dragon curling, and the Stone Mountain, a tiger crouching”. Mr. Sun Yet-sen spoke highly of Nanjing in the “Constructive Scheme for Our Country”, “The position of Nanjing is wonderful since mountains, lakes and plains all integrated in it. It is hard to find another city like this.”\n",
      "Question:What mountain lies in the west of Nanjing?\n",
      "Answer:['Stone Mountain']\n",
      "Answer Start in Text:[369]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:Investment in the city continued. The William Enston Home, a planned community for the city's aged and infirm, was built in 1889. An elaborate public building, the United States Post Office and Courthouse, was completed by the federal government in 1896 in the heart of the city. The Democrat-dominated state legislature passed a new constitution in 1895 that disfranchised blacks, effectively excluding them entirely from the political process, a second-class status that was maintained for more than six decades in a state that was majority black until about 1930.\n",
      "Question:A large post office and courthouse was built in what year?\n",
      "Answer:['1896']\n",
      "Answer Start in Text:[249]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:The concept of 'education through recreation' was applied to childhood development in the 19th century. In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities. L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: \"A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself he always seems to be doing both. Enough for him that he does it well.\" Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities. The concept has been revived by the University of Western Ontario to teach anatomy to medical students.\n",
      "Question:What changed when the concept was broadened?\n",
      "Answer:['to include young adults']\n",
      "Answer Start in Text:[157]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:In contrast, the ROK Army defenders were relatively unprepared and ill-equipped. In South to the Naktong, North to the Yalu (1961), R.E. Appleman reports the ROK forces' low combat readiness as of 25 June 1950. The ROK Army had 98,000 soldiers (65,000 combat, 33,000 support), no tanks (they had been requested from the U.S. military, but requests were denied), and a 22-piece air force comprising 12 liaison-type and 10 AT6 advanced-trainer airplanes. There were no large foreign military garrisons in Korea at the time of the invasion, but there were large U.S. garrisons and air forces in Japan.\n",
      "Question:In what country did the US maintain air forces and garrisons?\n",
      "Answer:['Japan']\n",
      "Answer Start in Text:[592]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"squad\")\n",
    "def visualize(datasets, datatype = 'train', n_questions=10):\n",
    "    n = len(datasets[datatype])\n",
    "    random_questions=random.choices(list(range(n)),k=n_questions)\n",
    "    for i in random_questions:\n",
    "        print(f\"Context:{datasets[datatype][i]['context']}\")\n",
    "        print(f\"Question:{datasets[datatype][i]['question']}\")\n",
    "        print(f\"Answer:{datasets[datatype][i]['answers']['text']}\")\n",
    "        print(f\"Answer Start in Text:{datasets[datatype][i]['answers']['answer_start']}\")\n",
    "        print(\"-\"*100)\n",
    "visualize(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "789412a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aec7338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18be9083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57341b484776f41900661888',\n",
       " 'title': 'Montana',\n",
       " 'context': 'Montana is home to a diverse array of fauna that includes 14 amphibian, 90 fish, 117 mammal, 20 reptile and 427 bird species. Additionally, there are over 10,000 invertebrate species, including 180 mollusks and 30 crustaceans. Montana has the largest grizzly bear population in the lower 48 states. Montana hosts five federally endangered species–black-footed ferret, whooping crane, least tern, pallid sturgeon and white sturgeon and seven threatened species including the grizzly bear, Canadian lynx and bull trout. The Montana Department of Fish, Wildlife and Parks manages fishing and hunting seasons for at least 17 species of game fish including seven species of trout, walleye and smallmouth bass and at least 29 species of game birds and animals including ring-neck pheasant, grey partridge, elk, pronghorn antelope, mule deer, whitetail deer, gray wolf and bighorn sheep.',\n",
       " 'question': 'How many species of game fish have hunting seasons?',\n",
       " 'answers': {'text': ['at least 17'], 'answer_start': [609]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f827c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b30de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"squad\")\n",
    "def visualize(datasets, datatype = 'train', n_questions=10):\n",
    "    n = len(datasets[datatype])\n",
    "    random_questions=random.choices(list(range(n)),k=n_questions)\n",
    "    for i in random_questions:\n",
    "        print(f\"Context:{datasets[datatype][i]['context']}\")\n",
    "        print(f\"Question:{datasets[datatype][i]['question']}\")\n",
    "        print(f\"Answer:{datasets[datatype][i]['answers']['text']}\")\n",
    "        print(f\"Answer Start in Text:{datasets[datatype][i]['answers']['answer_start']}\")\n",
    "        print(\"-\"*100)\n",
    "visualize(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
